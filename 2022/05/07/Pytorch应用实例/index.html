<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Pytorch实战# 1_dataset首先我们应该知道，这里的dataset是指什么？ 字面上翻译就是数据集，但其实在pytorch中定义的dataset并不是数据本身，而是一个个类的实例。 1.0 继承 torch.utils.data.Dataset对于dataset而言，它的基类就是torch.utils.data.Dataset 12import torchfrom torch.util">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch应用实例">
<meta property="og:url" content="https://herbstlicherwind.github.io/2022/05/07/Pytorch%E5%BA%94%E7%94%A8%E5%AE%9E%E4%BE%8B/index.html">
<meta property="og:site_name" content="清辉夜凝">
<meta property="og:description" content="Pytorch实战# 1_dataset首先我们应该知道，这里的dataset是指什么？ 字面上翻译就是数据集，但其实在pytorch中定义的dataset并不是数据本身，而是一个个类的实例。 1.0 继承 torch.utils.data.Dataset对于dataset而言，它的基类就是torch.utils.data.Dataset 12import torchfrom torch.util">
<meta property="og:locale">
<meta property="og:image" content="c:/Users/86166/AppData/Roaming/Typora/typora-user-images/image-20220429153249894.png">
<meta property="article:published_time" content="2022-05-07T10:48:16.000Z">
<meta property="article:modified_time" content="2022-05-10T11:53:17.057Z">
<meta property="article:author" content="Yinghao Wang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:/Users/86166/AppData/Roaming/Typora/typora-user-images/image-20220429153249894.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://herbstlicherwind.github.io/2022/05/07/Pytorch应用实例/"/>





  <title>Pytorch应用实例 | 清辉夜凝</title>
  








<meta name="generator" content="Hexo 6.0.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">清辉夜凝</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            日程表
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://herbstlicherwind.github.io/2022/05/07/Pytorch%E5%BA%94%E7%94%A8%E5%AE%9E%E4%BE%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="清辉夜凝">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Pytorch应用实例</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-05-07T18:48:16+08:00">
                2022-05-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Pytorch实战"><a href="#Pytorch实战" class="headerlink" title="Pytorch实战"></a>Pytorch实战</h1><h2 id="1-dataset"><a href="#1-dataset" class="headerlink" title="# 1_dataset"></a><code># 1_dataset</code></h2><p>首先我们应该知道，这里的<strong>dataset</strong>是指什么？ 字面上翻译就是<strong>数据集</strong>，但其实在pytorch中定义的dataset并不是数据本身，而是一个个类的实例。</p>
<h3 id="1-0-继承-torch-utils-data-Dataset"><a href="#1-0-继承-torch-utils-data-Dataset" class="headerlink" title="1.0 继承 torch.utils.data.Dataset"></a>1.0 <strong>继承 <code>torch.utils.data.Dataset</code></strong></h3><p>对于dataset而言，它的基类就是torch.utils.data.Dataset</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></table></figure>
<h3 id="1-1-实现-getitem-和-len-两个magic-methods"><a href="#1-1-实现-getitem-和-len-两个magic-methods" class="headerlink" title="1.1 实现 __getitem__ 和 __len__ 两个magic methods"></a>1.1 实现 <code>__getitem__</code> 和 <code>__len__</code> 两个magic methods</h3><p>定义一些方法，能根据索引将数据集从文件夹中取得：利用<code>__getitem__</code> 和 <code>__len__</code>，需要由自己<strong>重写</strong>定义。<code>__getitem__</code> 返回数据内容，<code>__len__</code> 返回数据量。</p>
<p>下面我们主要看函数<code>eg_1_1</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Sampler</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_1_1</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg1.1 : __getitem__, __len__</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  x = torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">  y = x**<span class="number">2</span>  <span class="comment">#输入两个简单的数据</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">SimpleDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">      <span class="built_in">super</span>().__init__()</span><br><span class="line">      self.x = x</span><br><span class="line">      self.y = y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">      <span class="keyword">return</span> &#123;<span class="string">&quot;x&quot;</span>:self.x[index], <span class="string">&quot;y&quot;</span>:self.y[index]&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">len</span>(self.x)</span><br><span class="line"></span><br><span class="line">  simpledataset = SimpleDataset(x, y)</span><br><span class="line">  index = <span class="number">0</span></span><br><span class="line">  <span class="comment"># __getitem__</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;simpledataset.__getitem__(&#123;&#125;): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, simpledataset.__getitem__(index)))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;simpledataset[&#123;&#125;]: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, simpledataset[index]))</span><br><span class="line">  <span class="comment"># __len__</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;simpledataset.__len__(): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(simpledataset.__len__()))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;len(simpledataset): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(simpledataset)))</span><br><span class="line"><span class="comment">#以上可以发现getitem和len可以用更简洁的方式实现</span></span><br><span class="line">eg_1_1()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">simpledataset.__getitem__(0): &#123;&#x27;x&#x27;: tensor(-1.), &#x27;y&#x27;: tensor(1.)&#125;</span><br><span class="line">simpledataset[0]: &#123;&#x27;x&#x27;: tensor(-1.), &#x27;y&#x27;: tensor(1.)&#125;</span><br><span class="line">simpledataset.__len__(): 10</span><br><span class="line">len(simpledataset): 10</span><br><span class="line">#以上可以发现getitem和len可以用更简洁的方式实现</span><br></pre></td></tr></table></figure>
<h3 id="1-2-理解-MNIST-类，以及-transforms-模块"><a href="#1-2-理解-MNIST-类，以及-transforms-模块" class="headerlink" title="1.2 理解 MNIST 类，以及 transforms 模块"></a>1.2 <strong>理解 <code>MNIST</code> 类，以及 <code>transforms</code> 模块</strong></h3><p>下面以数据集中的’’helloworld”：MNIST类为例进行介绍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_1_2_0</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg1.2.0 : MNIST</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torchvision.datasets.mnist <span class="keyword">import</span> MNIST</span><br><span class="line">  train_dataset = MNIST(root=<span class="string">&quot;./mnist_data&quot;</span>,</span><br><span class="line">                        train=<span class="literal">True</span>,</span><br><span class="line">                        transform=<span class="literal">None</span>,</span><br><span class="line">                        download=<span class="literal">False</span>)<span class="comment">#实际上下载了二进制文件，在编辑器中不会显示，但是问题不大，相当于定义了getitem和len和方法，可以帮我们直接进行取用想要的数据。</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(train_dataset): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">type</span>(train_dataset)))  </span><br><span class="line">    <span class="comment"># &lt;class &#x27;torchvision.datasets.mnist.MNIST&#x27;&gt;</span></span><br><span class="line">  index = <span class="number">0</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;train_dataset[&#123;&#125;]: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, train_dataset[index]))  <span class="comment"># (PIL.Image.Image, 5)</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;len(train_dataset): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(train_dataset)))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">  %matplotlib inline</span><br><span class="line">  plt.imshow(train_dataset[index][<span class="number">0</span>], cmap =<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line">eg_1_2_0()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">type(train_dataset): &lt;class &#x27;torchvision.datasets.mnist.MNIST&#x27;&gt;</span><br><span class="line">train_dataset[0]: (&lt;PIL.Image.Image image mode=L size=28x28 at 0x7FF837B97640&gt;, 5)</span><br><span class="line">len(train_dataset): 60000 #MNIST中包含了60000张这样的图片</span><br></pre></td></tr></table></figure>
<p><img src="C:/Users/86166/AppData/Roaming/Typora/typora-user-images/image-20220429153249894.png" alt="image-20220429153249894" style="zoom:33%;"></p>
<p>这里可以得出我们从MNIST类里提取出来了一张灰度图片，但是我们需要的是tensor类型的数据，于是需要用transforms进行转换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_1_2_1</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg1.2.1 : transforms</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torchvision.datasets.mnist <span class="keyword">import</span> MNIST</span><br><span class="line">  <span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="comment">#torchvision提供了transforms模块，我们可以从中调用其中的类和方法将数据变成我们想要的格式。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#例如</span></span><br><span class="line">  transform = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">      transforms.ToTensor()，,<span class="comment">#将图片变成tensor类型</span></span><br><span class="line">      transforms.Normalize(mean=(<span class="number">0.5</span>,), std=(<span class="number">0.5</span>,))<span class="comment">#标准化到0到1之间的一个数</span></span><br><span class="line">    ]</span><br><span class="line">  )</span><br><span class="line">  train_dataset = MNIST(root=<span class="string">&quot;./mnist_data&quot;</span>,</span><br><span class="line">                        train=<span class="literal">True</span>,</span><br><span class="line">                        transform=transform,<span class="comment">#针对图片</span></span><br><span class="line">                        target_transform=<span class="literal">None</span>,<span class="comment">#针对标签 此处对标签不做改变</span></span><br><span class="line">                        download=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  index = <span class="number">0</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(train_dataset[&#123;&#125;]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, <span class="built_in">type</span>(train_dataset[index])))  <span class="comment"># &lt;class &#x27;tuple&#x27;&gt;</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(train_dataset[&#123;&#125;][0]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, <span class="built_in">type</span>(train_dataset[index][<span class="number">0</span>])))  <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;train_dataset[&#123;&#125;][0].shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, train_dataset[index][<span class="number">0</span>].shape))  <span class="comment"># torch.Size([1, 28, 28])</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(train_dataset[&#123;&#125;][1]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, <span class="built_in">type</span>(train_dataset[index][<span class="number">1</span>])))  <span class="comment"># &lt;class &#x27;int&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">eg_1_2_1()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">type(train_dataset[0]): &lt;class &#x27;tuple&#x27;&gt;</span><br><span class="line">type(train_dataset[0][0]): &lt;class &#x27;torch.Tensor&#x27;&gt; #可以看出格式已经转换</span><br><span class="line">train_dataset[0][0].shape: torch.Size([1, 28, 28]) #第一个变量是通道数</span><br><span class="line">type(train_dataset[0][1]): &lt;class &#x27;int&#x27;&gt;</span><br></pre></td></tr></table></figure>
<h3 id="1-3-利用-torchvision-datasets-中的数据集"><a href="#1-3-利用-torchvision-datasets-中的数据集" class="headerlink" title="1.3 利用 torchvision.datasets 中的数据集"></a>1.3 <strong>利用 <code>torchvision.datasets</code> 中的数据集</strong></h3><p> <code>torchvision.datasets</code> 里面也提供了一些数据集，例如用于<strong>语义分割</strong>和<strong>目标检测</strong>的数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_1_3</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg1.3 : VOCSegmentation, VOCDetection</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torchvision.datasets.voc <span class="keyword">import</span> VOCSegmentation, VOCDetection</span><br><span class="line"></span><br><span class="line">  segmentation_dataset = VOCSegmentation(root=<span class="string">&quot;./voc_data&quot;</span>,</span><br><span class="line">                                        image_set=<span class="string">&quot;train&quot;</span>,</span><br><span class="line">                                        transform=<span class="literal">None</span>,</span><br><span class="line">                                        download=<span class="literal">False</span>)<span class="comment">#语义分割数据集</span></span><br><span class="line">  detection_dataset = VOCDetection(root=<span class="string">&quot;./voc_data&quot;</span>,</span><br><span class="line">                                  image_set=<span class="string">&quot;train&quot;</span>,</span><br><span class="line">                                  transform=<span class="literal">None</span>,</span><br><span class="line">                                  download=<span class="literal">False</span>)<span class="comment">#目标检测数据集</span></span><br><span class="line"></span><br><span class="line">  index = <span class="number">0</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(segmentation_dataset[&#123;&#125;]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, <span class="built_in">type</span>(segmentation_dataset[index])))  <span class="comment"># &lt;class &#x27;tuple&#x27;&gt;</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(segmentation_dataset[&#123;&#125;][0]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, <span class="built_in">type</span>(segmentation_dataset[index][<span class="number">0</span>])))  <span class="comment"># &lt;class &#x27;PIL.Image.Image&#x27;&gt;</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(segmentation_dataset[&#123;&#125;][1]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, <span class="built_in">type</span>(segmentation_dataset[index][<span class="number">1</span>])))  <span class="comment"># &lt;class &#x27;PIL.PngImagePlugin.PngImageFile&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(detection_dataset[&#123;&#125;]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, <span class="built_in">type</span>(detection_dataset[index])))  <span class="comment"># &lt;class &#x27;tuple&#x27;&gt;</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(detection_dataset[&#123;&#125;][0]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, <span class="built_in">type</span>(detection_dataset[index][<span class="number">0</span>])))  <span class="comment"># &lt;class &#x27;PIL.Image.Image&#x27;&gt;</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(detection_dataset[&#123;&#125;][1]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, <span class="built_in">type</span>(detection_dataset[index][<span class="number">1</span>])))  <span class="comment"># &lt;class &#x27;dict&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line">eg_1_3()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">type(segmentation_dataset[0]): &lt;class &#x27;tuple&#x27;&gt;</span><br><span class="line">type(segmentation_dataset[0][0]): &lt;class &#x27;PIL.Image.Image&#x27;&gt;</span><br><span class="line">type(segmentation_dataset[0][1]): &lt;class &#x27;PIL.PngImagePlugin.PngImageFile&#x27;&gt;</span><br><span class="line">type(detection_dataset[0]): &lt;class &#x27;tuple&#x27;&gt;</span><br><span class="line">type(detection_dataset[0][0]): &lt;class &#x27;PIL.Image.Image&#x27;&gt;</span><br><span class="line">type(detection_dataset[0][1]): &lt;class &#x27;dict&#x27;&gt;</span><br></pre></td></tr></table></figure>
<h3 id="1-4-理解-ImageFolder-类及其-classes-与-class-to-idx-属性"><a href="#1-4-理解-ImageFolder-类及其-classes-与-class-to-idx-属性" class="headerlink" title="1.4 理解 ImageFolder 类及其 classes 与 class_to_idx 属性"></a>1.4 <strong>理解 <code>ImageFolder</code> 类及其 <code>classes</code> 与 <code>class_to_idx</code> 属性</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_1_4_0</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg1.4.0 : ImageFolder</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> ImageFolder</span><br><span class="line">  <span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">  transform = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">      transforms.RandomResizedCrop(size=(<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">      transforms.RandomHorizontalFlip(),</span><br><span class="line">      transforms.ToTensor(),<span class="comment">##</span></span><br><span class="line">      transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">    ]</span><br><span class="line">  )<span class="comment">#首先把png格式的数据转换成tensor形式</span></span><br><span class="line">  train_dataset = ImageFolder(root=os.path.join(<span class="string">&quot;./flower_data&quot;</span>, <span class="string">&quot;train&quot;</span>), transform=transform)</span><br><span class="line"></span><br><span class="line">  index = <span class="number">0</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(train_dataset[&#123;&#125;]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, <span class="built_in">type</span>(train_dataset[index])))  <span class="comment"># &lt;class &#x27;tuple&#x27;&gt;</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(train_dataset[&#123;&#125;][0]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, <span class="built_in">type</span>(train_dataset[index][<span class="number">0</span>])))  <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;train_dataset[&#123;&#125;][0].shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, train_dataset[index][<span class="number">0</span>].shape))  <span class="comment"># torch.Size([3, 224, 224])</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(train_dataset[&#123;&#125;][1]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(index, <span class="built_in">type</span>(train_dataset[index][<span class="number">1</span>])))  <span class="comment"># &lt;class &#x27;int&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line">eg_1_4_0()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">type(train_dataset[0]): &lt;class &#x27;tuple&#x27;&gt;</span><br><span class="line">type(train_dataset[0][0]): &lt;class &#x27;torch.Tensor&#x27;&gt;</span><br><span class="line">train_dataset[0][0].shape: torch.Size([3, 224, 224])</span><br><span class="line">type(train_dataset[0][1]): &lt;class &#x27;int&#x27;&gt;</span><br></pre></td></tr></table></figure>
<p>截止到此，我们已经对dataset有了初步的了解，并不是文件夹里面的数据本身，而是一个个类的实例；它提供了一些方法帮助我们获得我们所需要的数据；同时我们可以使用transforms进行数据的格式转换。</p>
<h2 id="2-dataloader"><a href="#2-dataloader" class="headerlink" title="# 2_dataloader"></a><code># 2_dataloader</code></h2><h3 id="2-0-利用-torch-utils-data-DataLoader类"><a href="#2-0-利用-torch-utils-data-DataLoader类" class="headerlink" title="2.0 利用 torch.utils.data.DataLoader类"></a>2.0 <strong>利用 <code>torch.utils.data.DataLoader</code>类</strong></h3><p>DataLoader也是一个类，官方的定义是数据加载器。组合数据集和采样器，并提供给定数据集上的iterable。</p>
<p>iterable指的就是一个可迭代的对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data._utils <span class="keyword">import</span> collate</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets.mnist <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">  [</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=(<span class="number">0.5</span>,), std=(<span class="number">0.5</span>,))</span><br><span class="line">  ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_dataset = MNIST(root=<span class="string">&quot;./mnist_data&quot;</span>,</span><br><span class="line">                      train=<span class="literal">True</span>,</span><br><span class="line">                      transform=transform,</span><br><span class="line">                      target_transform=<span class="literal">None</span>,</span><br><span class="line">                      download=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#以上是已经将dataset定义完成                   </span></span><br></pre></td></tr></table></figure>
<h3 id="2-1-理解-iter-这个magic-method"><a href="#2-1-理解-iter-这个magic-method" class="headerlink" title="2.1 理解 __iter__ 这个magic method"></a>2.1 <strong>理解 <code>__iter__</code> 这个magic method</strong></h3><p>类似于getitem和len，iter也是返回一个对象，即实现的是返回的是iterable的对象，具体方法可以在源码中查找。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_2_1</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg2.1 : __iter__</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">  train_loader = DataLoader(dataset=train_dataset,</span><br><span class="line">                            batch_size=<span class="number">10000</span>,</span><br><span class="line">                            shuffle=<span class="literal">False</span><span class="comment">#每次都按照固定顺序，不同的epoch之间的数据不会打乱)</span></span><br><span class="line">                            <span class="comment">#将数据集按照给定的批量大小进行划分</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;type(train_loader): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">type</span>(train_loader)))  <span class="comment"># &lt;class &#x27;torch.utils.data.dataloader.DataLoader&#x27;&gt;</span></span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:<span class="comment">#我们将每个划分的批次进行遍历</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;type(batch): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">type</span>(batch)))  <span class="comment"># &lt;class &#x27;list&#x27;&gt;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;len(batch): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(batch)))  <span class="comment"># 2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;type(batch[0]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">type</span>(batch[<span class="number">0</span>])))  <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;type(batch[1]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">type</span>(batch[<span class="number">0</span>])))  <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;batch[0].shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(batch[<span class="number">0</span>].shape))  <span class="comment"># torch.Size([10000, 1, 28, 28])</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;batch[1].shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(batch[<span class="number">1</span>].shape))  <span class="comment"># torch.Size([10000])</span></span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">eg_2_1()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">type(train_loader): &lt;class &#x27;torch.utils.data.dataloader.DataLoader&#x27;&gt;</span><br><span class="line">type(batch): &lt;class &#x27;list&#x27;&gt;</span><br><span class="line">len(batch): 2</span><br><span class="line">type(batch[0]): &lt;class &#x27;torch.Tensor&#x27;&gt;</span><br><span class="line">type(batch[1]): &lt;class &#x27;torch.Tensor&#x27;&gt;</span><br><span class="line">batch[0].shape: torch.Size([10000, 1, 28, 28])</span><br><span class="line">batch[1].shape: torch.Size([10000])</span><br></pre></td></tr></table></figure>
<h3 id="2-2-区分-Dataloader-与-Dataset-的-len"><a href="#2-2-区分-Dataloader-与-Dataset-的-len" class="headerlink" title="2.2 区分 Dataloader 与 Dataset 的 __len__"></a>2.2 <strong>区分 <code>Dataloader</code> 与 <code>Dataset</code> 的 <code>__len__</code></strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_2_2</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg2.2 : __len__</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">  train_loader = DataLoader(dataset=train_dataset,</span><br><span class="line">                            batch_size=<span class="number">10000</span>,</span><br><span class="line">                            shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;len(train_loader): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(train_loader)))  <span class="comment"># 6</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;len(train_loader.dataset): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(train_loader.dataset)))  <span class="comment"># 60000</span></span><br><span class="line"></span><br><span class="line">eg_2_2()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">len(train_loader): 6 #len(train_loader.dataset)的值是整个数据集的数量，而len(train_loader)是由我们人为划分出来的</span><br><span class="line">len(train_loader.dataset): 60000</span><br></pre></td></tr></table></figure>
<h3 id="2-3-利用-内置函数-enumerate-与-tqdm-模块"><a href="#2-3-利用-内置函数-enumerate-与-tqdm-模块" class="headerlink" title="2.3 利用 内置函数 enumerate 与 tqdm 模块"></a>2.3 利用 内置函数 <code>enumerate</code> 与 <code>tqdm</code> 模块</h3><p>这里我们介绍一个python的内置函数enumerate，它能将可迭代的对象同时返回它的下标索引和数据本身</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_2_3_0</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg2.3.0 : enumerate</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">  train_loader = DataLoader(dataset=train_dataset,</span><br><span class="line">                            batch_size=<span class="number">10000</span>,</span><br><span class="line">                            shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> batch, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):<span class="comment">#batch就是下标索引，enumerate(train_loader)就是数据本身；</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;batch: &#123;&#125;, type(x): &#123;&#125;, type(y): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(batch, <span class="built_in">type</span>(x), <span class="built_in">type</span>(y)))</span><br><span class="line">    <span class="comment"># batch: 0, type(x): &lt;class &#x27;torch.Tensor&#x27;&gt;, type(y): &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;batch: &#123;&#125;, x.shape: &#123;&#125;, y.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(batch, x.shape, y.shape))</span><br><span class="line">    <span class="comment"># batch: 0, x.shape: torch.Size([10000, 1, 28, 28]), y.shape: torch.Size([10000])</span></span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">eg_2_3_0()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch: 0, type(x): &lt;class &#x27;torch.Tensor&#x27;&gt;, type(y): &lt;class &#x27;torch.Tensor&#x27;&gt;</span><br><span class="line">batch: 0, x.shape: torch.Size([10000, 1, 28, 28]), y.shape: torch.Size([10000])</span><br></pre></td></tr></table></figure>
<p>以及为了让训练过程更加可视化，引入了tqdm模块（就是把训练过程变成进度条化了）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_2_3_1</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg2.3.1 : tqdm</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">  <span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line">  train_loader = DataLoader(dataset=train_dataset,</span><br><span class="line">                            batch_size=<span class="number">10000</span>,</span><br><span class="line">                            shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tqdm(train_loader, desc=<span class="string">&quot;TRAINING&quot;</span>) <span class="keyword">as</span> train_bar:</span><br><span class="line">    <span class="keyword">for</span> (x, y) <span class="keyword">in</span> train_bar:</span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">eg_2_3_1()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TRAINING: 100%|██████████| 6/6 [00:11&lt;00:00,  1.89s/it]</span><br></pre></td></tr></table></figure>
<h3 id="2-3-另外有需要可以更改-collate-fn"><a href="#2-3-另外有需要可以更改-collate-fn" class="headerlink" title="2.3 另外有需要可以更改 collate_fn"></a>2.3 另外有需要可以更改 <code>collate_fn</code></h3><p>对于collate_fn的理解可以看这个<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43391414/article/details/120462055">pytorch之深入理解collate_fn</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_2_4</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg2.4 : collate_fn</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">batch</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;type(batch): &#123;&#125;, len(batch): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">type</span>(batch), <span class="built_in">len</span>(batch)))  <span class="comment"># &lt;class &#x27;list&#x27;&gt;, 10000</span></span><br><span class="line">    x = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> batch]</span><br><span class="line">    y = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> batch]</span><br><span class="line">    x = torch.cat(x)[:,<span class="literal">None</span>,...]</span><br><span class="line">    y = torch.Tensor(y)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;x&quot;</span>:x, <span class="string">&quot;y&quot;</span>:y&#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">  <span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line">  train_loader = DataLoader(dataset=train_dataset,</span><br><span class="line">                            batch_size=<span class="number">10000</span>,</span><br><span class="line">                            shuffle=<span class="literal">False</span>,</span><br><span class="line">                            collate_fn=collate_fn)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;type(batch): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">type</span>(batch)))  <span class="comment"># &lt;class &#x27;dict&#x27;&gt;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;type(batch[\&quot;x\&quot;]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">type</span>(batch[<span class="string">&quot;x&quot;</span>])))  <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;type(batch[\&quot;y\&quot;]): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">type</span>(batch[<span class="string">&quot;y&quot;</span>])))  <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;batch[\&quot;x\&quot;].shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(batch[<span class="string">&quot;x&quot;</span>].shape))  <span class="comment"># torch.Size([10000, 1, 28, 28])</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;batch[\&quot;y\&quot;].shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(batch[<span class="string">&quot;y&quot;</span>].shape))  <span class="comment"># torch.Size([10000])</span></span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">eg_2_4()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">type(batch): &lt;class &#x27;list&#x27;&gt;, len(batch): 10000</span><br><span class="line">type(batch): &lt;class &#x27;dict&#x27;&gt;</span><br><span class="line">type(batch[&quot;x&quot;]): &lt;class &#x27;torch.Tensor&#x27;&gt;</span><br><span class="line">type(batch[&quot;y&quot;]): &lt;class &#x27;torch.Tensor&#x27;&gt;</span><br><span class="line">batch[&quot;x&quot;].shape: torch.Size([10000, 1, 28, 28])</span><br><span class="line">batch[&quot;y&quot;].shape: torch.Size([10000])</span><br></pre></td></tr></table></figure>
<h2 id="3-model"><a href="#3-model" class="headerlink" title="# 3_model"></a><code># 3_model</code></h2><h3 id="3-0-继承-torch-nn-Module，注意-super-init"><a href="#3-0-继承-torch-nn-Module，注意-super-init" class="headerlink" title="3.0 继承 torch.nn.Module，注意 super().__init__()"></a>3.0 继承 <code>torch.nn.Module</code>，注意 <code>super().__init__()</code></h3><p>model的基类是torch.nn.Module</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader, dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models, transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets.mnist <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">  [</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=(<span class="number">0.5</span>,), std=(<span class="number">0.5</span>,))</span><br><span class="line">  ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_dataset = MNIST(root=<span class="string">&quot;./mnist_data&quot;</span>,</span><br><span class="line">                      train=<span class="literal">True</span>,</span><br><span class="line">                      transform=transform,</span><br><span class="line">                      target_transform=<span class="literal">None</span>,</span><br><span class="line">                      download=<span class="literal">False</span>)</span><br><span class="line">train_loader = DataLoader(dataset=train_dataset,</span><br><span class="line">                          batch_size=<span class="number">10000</span>,</span><br><span class="line">                          shuffle=<span class="literal">True</span>)</span><br><span class="line">                          </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_3_0_0</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg3.0.0 : torch.nn.Module</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">    <span class="comment">#下面定义一个很简单的model</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">SimpleModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SimpleModel, self).__init__() </span><br><span class="line">        <span class="comment">#实现了父类的_init_的方法， 如果没有这句 就会报错，从Module源码中可以了解到具体原因。</span></span><br><span class="line">        </span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">3</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>))  <span class="comment">#卷积层</span></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>) <span class="comment">#激活函数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">  model = SimpleModel()</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;model: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(model))</span><br><span class="line">  <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name, param)</span><br><span class="line">eg_3_0_0()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model: SimpleModel(</span><br><span class="line">  (conv1): Conv2d(1, 3, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">  (relu): ReLU(inplace=True)</span><br><span class="line">)  #打印出的model框架</span><br><span class="line">conv1.weight Parameter containing:</span><br><span class="line">tensor([[[[-0.6109]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.3560]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.7688]]]], requires_grad=True)</span><br><span class="line">conv1.bias Parameter containing:</span><br><span class="line">tensor([0.3276, 0.4379, 0.1914], requires_grad=True)</span><br></pre></td></tr></table></figure>
<h3 id="3-1-理解-call-这个magic-method-与自定义-forward-关系"><a href="#3-1-理解-call-这个magic-method-与自定义-forward-关系" class="headerlink" title="3.1 理解 __call__ 这个magic method 与自定义 forward 关系"></a>3.1 理解 <code>__call__</code> 这个magic method 与自定义 <code>forward</code> 关系</h3><p><strong> </strong>call<strong>_</strong>也是魔法motheds，在类中实现这一方法可以使改类的实例（对象）像函数一样被调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_3_1</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg3.1 : __call__  [magic methods]</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">SimpleModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SimpleModel, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">3</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">  <span class="comment">#定义model类</span></span><br><span class="line">  model = SimpleModel()</span><br><span class="line">  x = train_dataset[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># torch.Size([1, 28, 28])</span></span><br><span class="line">  x = x[<span class="literal">None</span>, ...]  <span class="comment"># torch.Size([1, 1, 28, 28])</span></span><br><span class="line">  <span class="built_in">print</span>(model(x) == model.forward(x))</span><br><span class="line"> <span class="comment">#print(model(x) == model.__call__(x)) 用魔法函数 输出结果是一样的 可以让我们使用函数一样使用model</span></span><br><span class="line">eg_3_1()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[True, True, True,  ..., True, True, True],</span><br><span class="line">          [True, True, True,  ..., True, True, True],</span><br><span class="line">          [True, True, True,  ..., True, True, True],</span><br><span class="line">          ...,</span><br><span class="line">          [True, True, True,  ..., True, True, True],</span><br><span class="line">          [True, True, True,  ..., True, True, True],</span><br><span class="line">          [True, True, True,  ..., True, True, True]],</span><br><span class="line"></span><br><span class="line">         [[True, True, True,  ..., True, True, True],</span><br><span class="line">          [True, True, True,  ..., True, True, True],</span><br><span class="line">          [True, True, True,  ..., True, True, True],</span><br><span class="line">          ...,</span><br><span class="line">          [True, True, True,  ..., True, True, True],</span><br><span class="line">          [True, True, True,  ..., True, True, True],</span><br><span class="line">          [True, True, True,  ..., True, True, True]],</span><br><span class="line"></span><br><span class="line">         [[True, True, True,  ..., True, True, True],</span><br><span class="line">          [True, True, True,  ..., True, True, True],</span><br><span class="line">          [True, True, True,  ..., True, True, True],</span><br><span class="line">          ...,</span><br><span class="line">          [True, True, True,  ..., True, True, True],</span><br><span class="line">          [True, True, True,  ..., True, True, True],</span><br><span class="line">          [True, True, True,  ..., True, True, True]]]])</span><br></pre></td></tr></table></figure>
<h3 id="3-2-注意-PyTorch-中数据的摆放-B-C-H-W"><a href="#3-2-注意-PyTorch-中数据的摆放-B-C-H-W" class="headerlink" title="3.2 注意 PyTorch 中数据的摆放 (B, C, H ,W)"></a>3.2 注意 <code>PyTorch</code> 中数据的摆放 <code>(B, C, H ,W)</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_3_2</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg3.2 : (B, C, H ,W)</span></span><br><span class="line"><span class="string">  [batch_size , channels，hight， width]</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">SimpleModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SimpleModel, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">3</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>))<span class="comment">#卷积</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">5</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>))<span class="comment">#卷积x2</span></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)<span class="comment">#激活</span></span><br><span class="line">        self.flatten = nn.Flatten(start_dim=<span class="number">1</span>, end_dim=-<span class="number">1</span>)<span class="comment">#拉平 里面的参数代表是从C到W</span></span><br><span class="line">        self.linear = nn.Linear(in_features=<span class="number">5</span>*<span class="number">28</span>*<span class="number">28</span>, out_features=<span class="number">10</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[before flatten] x.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(x.shape))  <span class="comment"># torch.Size([1, 5, 28, 28])</span></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[after flatten] x.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(x.shape))  <span class="comment"># torch.Size([1, 3920])</span></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">  model = SimpleModel()</span><br><span class="line">  x = train_dataset[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># torch.Size([1, 28, 28])</span></span><br><span class="line">  x = x[<span class="literal">None</span>, ...]  <span class="comment"># torch.Size([1, 1, 28, 28])#先扩充一维再输入 因为原本数据只有CHW</span></span><br><span class="line">  model(x) </span><br><span class="line"> </span><br><span class="line">eg_3_2()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[before flatten] x.shape: torch.Size([1, 5, 28, 28])</span><br><span class="line">[after flatten] x.shape: torch.Size([1, 3920])</span><br><span class="line">#拉平前后对比 </span><br></pre></td></tr></table></figure>
<h3 id="3-3-调用-torchvison-models-中现成的网络"><a href="#3-3-调用-torchvison-models-中现成的网络" class="headerlink" title="3.3 调用 torchvison.models 中现成的网络"></a>3.3 调用 <code>torchvison.models</code> 中现成的网络</h3><p>这个库里面有很多经典的网络，复现经典论文时可以参考原码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_3_3</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg3.3 : torchvision.models</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line">  model_vgg16 = models.vgg16()</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;model_vgg16: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(model_vgg16))</span><br><span class="line"></span><br><span class="line">  model_resnet50 = models.resnet50()</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;model_resnet50: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(model_resnet50))</span><br><span class="line"></span><br><span class="line">eg_3_3()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br></pre></td><td class="code"><pre><span class="line">model_vgg16: VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (3): ReLU(inplace=True)</span><br><span class="line">    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (6): ReLU(inplace=True)</span><br><span class="line">    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (8): ReLU(inplace=True)</span><br><span class="line">    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (13): ReLU(inplace=True)</span><br><span class="line">    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (15): ReLU(inplace=True)</span><br><span class="line">    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (18): ReLU(inplace=True)</span><br><span class="line">    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (20): ReLU(inplace=True)</span><br><span class="line">    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (22): ReLU(inplace=True)</span><br><span class="line">    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (25): ReLU(inplace=True)</span><br><span class="line">    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (27): ReLU(inplace=True)</span><br><span class="line">    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (29): ReLU(inplace=True)</span><br><span class="line">    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Linear(in_features=25088, out_features=4096, bias=True)</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (3): Linear(in_features=4096, out_features=4096, bias=True)</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (6): Linear(in_features=4096, out_features=1000, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">model_resnet50: ResNet(</span><br><span class="line">  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)</span><br><span class="line">  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (relu): ReLU(inplace=True)</span><br><span class="line">  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer2): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (3): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer3): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (3): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (4): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (5): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer4): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))</span><br><span class="line">  (fc): Linear(in_features=2048, out_features=1000, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="3-4-注意-torch-nn-Module-dict-state-torch-save-torch-load-以及-torch-nn-Module-load-state-dict-及其中参数"><a href="#3-4-注意-torch-nn-Module-dict-state-torch-save-torch-load-以及-torch-nn-Module-load-state-dict-及其中参数" class="headerlink" title="3.4 注意 torch.nn.Module.dict_state() torch.save() torch.load() 以及 torch.nn.Module.load_state_dict() 及其中参数"></a>3.4 注意 <code>torch.nn.Module.dict_state()</code> <code>torch.save()</code> <code>torch.load()</code> 以及 <code>torch.nn.Module.load_state_dict()</code> 及其中参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_3_4_0</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg3.4.0 : model.state_dict()</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line">  model_vgg16 = models.vgg16()</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;model_vgg16.state_dict():&#123;&#125;&quot;</span>.<span class="built_in">format</span>(model_vgg16.state_dict()))</span><br><span class="line"><span class="comment">#返回一个有序的字典 获得模型参数</span></span><br><span class="line">eg_3_4_0()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br><span class="line">959</span><br><span class="line">960</span><br><span class="line">961</span><br><span class="line">962</span><br><span class="line">963</span><br><span class="line">964</span><br><span class="line">965</span><br><span class="line">966</span><br><span class="line">967</span><br><span class="line">968</span><br><span class="line">969</span><br><span class="line">970</span><br><span class="line">971</span><br><span class="line">972</span><br><span class="line">973</span><br><span class="line">974</span><br><span class="line">975</span><br><span class="line">976</span><br><span class="line">977</span><br><span class="line">978</span><br><span class="line">979</span><br><span class="line">980</span><br><span class="line">981</span><br><span class="line">982</span><br><span class="line">983</span><br><span class="line">984</span><br><span class="line">985</span><br><span class="line">986</span><br><span class="line">987</span><br><span class="line">988</span><br><span class="line">989</span><br><span class="line">990</span><br><span class="line">991</span><br><span class="line">992</span><br><span class="line">993</span><br><span class="line">994</span><br><span class="line">995</span><br><span class="line">996</span><br><span class="line">997</span><br><span class="line">998</span><br><span class="line">999</span><br><span class="line">1000</span><br><span class="line">1001</span><br><span class="line">1002</span><br><span class="line">1003</span><br><span class="line">1004</span><br><span class="line">1005</span><br><span class="line">1006</span><br><span class="line">1007</span><br><span class="line">1008</span><br><span class="line">1009</span><br><span class="line">1010</span><br><span class="line">1011</span><br><span class="line">1012</span><br><span class="line">1013</span><br><span class="line">1014</span><br><span class="line">1015</span><br><span class="line">1016</span><br><span class="line">1017</span><br><span class="line">1018</span><br><span class="line">1019</span><br><span class="line">1020</span><br><span class="line">1021</span><br><span class="line">1022</span><br><span class="line">1023</span><br><span class="line">1024</span><br><span class="line">1025</span><br><span class="line">1026</span><br><span class="line">1027</span><br><span class="line">1028</span><br><span class="line">1029</span><br><span class="line">1030</span><br><span class="line">1031</span><br><span class="line">1032</span><br><span class="line">1033</span><br><span class="line">1034</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">1041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line">1051</span><br><span class="line">1052</span><br><span class="line">1053</span><br><span class="line">1054</span><br><span class="line">1055</span><br><span class="line">1056</span><br><span class="line">1057</span><br><span class="line">1058</span><br><span class="line">1059</span><br><span class="line">1060</span><br><span class="line">1061</span><br><span class="line">1062</span><br><span class="line">1063</span><br><span class="line">1064</span><br><span class="line">1065</span><br><span class="line">1066</span><br><span class="line">1067</span><br><span class="line">1068</span><br><span class="line">1069</span><br><span class="line">1070</span><br><span class="line">1071</span><br><span class="line">1072</span><br><span class="line">1073</span><br><span class="line">1074</span><br><span class="line">1075</span><br><span class="line">1076</span><br><span class="line">1077</span><br><span class="line">1078</span><br><span class="line">1079</span><br><span class="line">1080</span><br><span class="line">1081</span><br><span class="line">1082</span><br><span class="line">1083</span><br><span class="line">1084</span><br><span class="line">1085</span><br><span class="line">1086</span><br><span class="line">1087</span><br><span class="line">1088</span><br><span class="line">1089</span><br><span class="line">1090</span><br><span class="line">1091</span><br><span class="line">1092</span><br><span class="line">1093</span><br><span class="line">1094</span><br><span class="line">1095</span><br><span class="line">1096</span><br><span class="line">1097</span><br><span class="line">1098</span><br><span class="line">1099</span><br><span class="line">1100</span><br><span class="line">1101</span><br><span class="line">1102</span><br><span class="line">1103</span><br><span class="line">1104</span><br><span class="line">1105</span><br><span class="line">1106</span><br><span class="line">1107</span><br><span class="line">1108</span><br><span class="line">1109</span><br><span class="line">1110</span><br><span class="line">1111</span><br><span class="line">1112</span><br><span class="line">1113</span><br><span class="line">1114</span><br><span class="line">1115</span><br><span class="line">1116</span><br><span class="line">1117</span><br><span class="line">1118</span><br><span class="line">1119</span><br><span class="line">1120</span><br><span class="line">1121</span><br><span class="line">1122</span><br><span class="line">1123</span><br><span class="line">1124</span><br><span class="line">1125</span><br><span class="line">1126</span><br><span class="line">1127</span><br><span class="line">1128</span><br><span class="line">1129</span><br><span class="line">1130</span><br><span class="line">1131</span><br><span class="line">1132</span><br><span class="line">1133</span><br><span class="line">1134</span><br><span class="line">1135</span><br><span class="line">1136</span><br><span class="line">1137</span><br><span class="line">1138</span><br><span class="line">1139</span><br><span class="line">1140</span><br><span class="line">1141</span><br><span class="line">1142</span><br><span class="line">1143</span><br><span class="line">1144</span><br><span class="line">1145</span><br><span class="line">1146</span><br><span class="line">1147</span><br><span class="line">1148</span><br><span class="line">1149</span><br><span class="line">1150</span><br><span class="line">1151</span><br><span class="line">1152</span><br><span class="line">1153</span><br><span class="line">1154</span><br><span class="line">1155</span><br><span class="line">1156</span><br><span class="line">1157</span><br><span class="line">1158</span><br><span class="line">1159</span><br><span class="line">1160</span><br><span class="line">1161</span><br><span class="line">1162</span><br><span class="line">1163</span><br><span class="line">1164</span><br><span class="line">1165</span><br><span class="line">1166</span><br><span class="line">1167</span><br><span class="line">1168</span><br><span class="line">1169</span><br><span class="line">1170</span><br><span class="line">1171</span><br><span class="line">1172</span><br><span class="line">1173</span><br><span class="line">1174</span><br><span class="line">1175</span><br><span class="line">1176</span><br><span class="line">1177</span><br><span class="line">1178</span><br><span class="line">1179</span><br><span class="line">1180</span><br><span class="line">1181</span><br><span class="line">1182</span><br><span class="line">1183</span><br><span class="line">1184</span><br><span class="line">1185</span><br><span class="line">1186</span><br><span class="line">1187</span><br><span class="line">1188</span><br><span class="line">1189</span><br><span class="line">1190</span><br><span class="line">1191</span><br><span class="line">1192</span><br><span class="line">1193</span><br><span class="line">1194</span><br><span class="line">1195</span><br><span class="line">1196</span><br><span class="line">1197</span><br><span class="line">1198</span><br><span class="line">1199</span><br><span class="line">1200</span><br><span class="line">1201</span><br><span class="line">1202</span><br><span class="line">1203</span><br><span class="line">1204</span><br><span class="line">1205</span><br><span class="line">1206</span><br><span class="line">1207</span><br><span class="line">1208</span><br><span class="line">1209</span><br><span class="line">1210</span><br><span class="line">1211</span><br><span class="line">1212</span><br><span class="line">1213</span><br><span class="line">1214</span><br><span class="line">1215</span><br><span class="line">1216</span><br><span class="line">1217</span><br><span class="line">1218</span><br><span class="line">1219</span><br><span class="line">1220</span><br><span class="line">1221</span><br><span class="line">1222</span><br><span class="line">1223</span><br><span class="line">1224</span><br><span class="line">1225</span><br><span class="line">1226</span><br><span class="line">1227</span><br><span class="line">1228</span><br><span class="line">1229</span><br><span class="line">1230</span><br><span class="line">1231</span><br><span class="line">1232</span><br><span class="line">1233</span><br><span class="line">1234</span><br><span class="line">1235</span><br><span class="line">1236</span><br><span class="line">1237</span><br><span class="line">1238</span><br><span class="line">1239</span><br><span class="line">1240</span><br><span class="line">1241</span><br><span class="line">1242</span><br><span class="line">1243</span><br><span class="line">1244</span><br><span class="line">1245</span><br><span class="line">1246</span><br><span class="line">1247</span><br><span class="line">1248</span><br><span class="line">1249</span><br><span class="line">1250</span><br><span class="line">1251</span><br><span class="line">1252</span><br><span class="line">1253</span><br><span class="line">1254</span><br><span class="line">1255</span><br><span class="line">1256</span><br><span class="line">1257</span><br><span class="line">1258</span><br><span class="line">1259</span><br><span class="line">1260</span><br><span class="line">1261</span><br><span class="line">1262</span><br><span class="line">1263</span><br><span class="line">1264</span><br><span class="line">1265</span><br><span class="line">1266</span><br><span class="line">1267</span><br><span class="line">1268</span><br><span class="line">1269</span><br><span class="line">1270</span><br><span class="line">1271</span><br><span class="line">1272</span><br><span class="line">1273</span><br><span class="line">1274</span><br><span class="line">1275</span><br><span class="line">1276</span><br><span class="line">1277</span><br><span class="line">1278</span><br><span class="line">1279</span><br><span class="line">1280</span><br><span class="line">1281</span><br><span class="line">1282</span><br><span class="line">1283</span><br><span class="line">1284</span><br><span class="line">1285</span><br><span class="line">1286</span><br><span class="line">1287</span><br><span class="line">1288</span><br><span class="line">1289</span><br><span class="line">1290</span><br><span class="line">1291</span><br><span class="line">1292</span><br><span class="line">1293</span><br><span class="line">1294</span><br><span class="line">1295</span><br><span class="line">1296</span><br><span class="line">1297</span><br><span class="line">1298</span><br><span class="line">1299</span><br><span class="line">1300</span><br><span class="line">1301</span><br><span class="line">1302</span><br><span class="line">1303</span><br><span class="line">1304</span><br><span class="line">1305</span><br><span class="line">1306</span><br><span class="line">1307</span><br><span class="line">1308</span><br><span class="line">1309</span><br><span class="line">1310</span><br><span class="line">1311</span><br><span class="line">1312</span><br><span class="line">1313</span><br><span class="line">1314</span><br><span class="line">1315</span><br><span class="line">1316</span><br><span class="line">1317</span><br><span class="line">1318</span><br><span class="line">1319</span><br><span class="line">1320</span><br><span class="line">1321</span><br><span class="line">1322</span><br><span class="line">1323</span><br><span class="line">1324</span><br><span class="line">1325</span><br><span class="line">1326</span><br><span class="line">1327</span><br><span class="line">1328</span><br><span class="line">1329</span><br><span class="line">1330</span><br><span class="line">1331</span><br><span class="line">1332</span><br><span class="line">1333</span><br><span class="line">1334</span><br><span class="line">1335</span><br><span class="line">1336</span><br><span class="line">1337</span><br><span class="line">1338</span><br><span class="line">1339</span><br><span class="line">1340</span><br><span class="line">1341</span><br><span class="line">1342</span><br><span class="line">1343</span><br><span class="line">1344</span><br><span class="line">1345</span><br><span class="line">1346</span><br><span class="line">1347</span><br><span class="line">1348</span><br><span class="line">1349</span><br><span class="line">1350</span><br><span class="line">1351</span><br><span class="line">1352</span><br><span class="line">1353</span><br><span class="line">1354</span><br><span class="line">1355</span><br><span class="line">1356</span><br><span class="line">1357</span><br><span class="line">1358</span><br><span class="line">1359</span><br><span class="line">1360</span><br><span class="line">1361</span><br><span class="line">1362</span><br><span class="line">1363</span><br><span class="line">1364</span><br><span class="line">1365</span><br><span class="line">1366</span><br><span class="line">1367</span><br><span class="line">1368</span><br><span class="line">1369</span><br><span class="line">1370</span><br><span class="line">1371</span><br><span class="line">1372</span><br><span class="line">1373</span><br><span class="line">1374</span><br><span class="line">1375</span><br><span class="line">1376</span><br><span class="line">1377</span><br><span class="line">1378</span><br><span class="line">1379</span><br><span class="line">1380</span><br><span class="line">1381</span><br><span class="line">1382</span><br><span class="line">1383</span><br><span class="line">1384</span><br><span class="line">1385</span><br><span class="line">1386</span><br><span class="line">1387</span><br><span class="line">1388</span><br><span class="line">1389</span><br><span class="line">1390</span><br><span class="line">1391</span><br><span class="line">1392</span><br><span class="line">1393</span><br><span class="line">1394</span><br><span class="line">1395</span><br><span class="line">1396</span><br><span class="line">1397</span><br><span class="line">1398</span><br><span class="line">1399</span><br><span class="line">1400</span><br><span class="line">1401</span><br><span class="line">1402</span><br><span class="line">1403</span><br><span class="line">1404</span><br><span class="line">1405</span><br><span class="line">1406</span><br><span class="line">1407</span><br><span class="line">1408</span><br><span class="line">1409</span><br><span class="line">1410</span><br><span class="line">1411</span><br><span class="line">1412</span><br><span class="line">1413</span><br><span class="line">1414</span><br><span class="line">1415</span><br><span class="line">1416</span><br><span class="line">1417</span><br><span class="line">1418</span><br><span class="line">1419</span><br><span class="line">1420</span><br><span class="line">1421</span><br><span class="line">1422</span><br><span class="line">1423</span><br><span class="line">1424</span><br><span class="line">1425</span><br><span class="line">1426</span><br><span class="line">1427</span><br><span class="line">1428</span><br><span class="line">1429</span><br><span class="line">1430</span><br><span class="line">1431</span><br><span class="line">1432</span><br><span class="line">1433</span><br><span class="line">1434</span><br><span class="line">1435</span><br><span class="line">1436</span><br><span class="line">1437</span><br><span class="line">1438</span><br><span class="line">1439</span><br><span class="line">1440</span><br><span class="line">1441</span><br><span class="line">1442</span><br><span class="line">1443</span><br><span class="line">1444</span><br><span class="line">1445</span><br><span class="line">1446</span><br><span class="line">1447</span><br><span class="line">1448</span><br><span class="line">1449</span><br><span class="line">1450</span><br><span class="line">1451</span><br><span class="line">1452</span><br><span class="line">1453</span><br><span class="line">1454</span><br><span class="line">1455</span><br><span class="line">1456</span><br><span class="line">1457</span><br><span class="line">1458</span><br><span class="line">1459</span><br><span class="line">1460</span><br><span class="line">1461</span><br><span class="line">1462</span><br><span class="line">1463</span><br><span class="line">1464</span><br><span class="line">1465</span><br><span class="line">1466</span><br><span class="line">1467</span><br><span class="line">1468</span><br><span class="line">1469</span><br><span class="line">1470</span><br><span class="line">1471</span><br><span class="line">1472</span><br><span class="line">1473</span><br><span class="line">1474</span><br><span class="line">1475</span><br><span class="line">1476</span><br><span class="line">1477</span><br><span class="line">1478</span><br><span class="line">1479</span><br><span class="line">1480</span><br><span class="line">1481</span><br><span class="line">1482</span><br><span class="line">1483</span><br><span class="line">1484</span><br><span class="line">1485</span><br><span class="line">1486</span><br><span class="line">1487</span><br><span class="line">1488</span><br><span class="line">1489</span><br><span class="line">1490</span><br><span class="line">1491</span><br><span class="line">1492</span><br><span class="line">1493</span><br><span class="line">1494</span><br><span class="line">1495</span><br><span class="line">1496</span><br><span class="line">1497</span><br><span class="line">1498</span><br><span class="line">1499</span><br><span class="line">1500</span><br><span class="line">1501</span><br><span class="line">1502</span><br><span class="line">1503</span><br><span class="line">1504</span><br><span class="line">1505</span><br><span class="line">1506</span><br><span class="line">1507</span><br><span class="line">1508</span><br><span class="line">1509</span><br><span class="line">1510</span><br><span class="line">1511</span><br><span class="line">1512</span><br><span class="line">1513</span><br><span class="line">1514</span><br><span class="line">1515</span><br><span class="line">1516</span><br><span class="line">1517</span><br><span class="line">1518</span><br><span class="line">1519</span><br><span class="line">1520</span><br><span class="line">1521</span><br><span class="line">1522</span><br><span class="line">1523</span><br><span class="line">1524</span><br><span class="line">1525</span><br><span class="line">1526</span><br><span class="line">1527</span><br><span class="line">1528</span><br><span class="line">1529</span><br><span class="line">1530</span><br><span class="line">1531</span><br><span class="line">1532</span><br><span class="line">1533</span><br><span class="line">1534</span><br><span class="line">1535</span><br><span class="line">1536</span><br><span class="line">1537</span><br><span class="line">1538</span><br><span class="line">1539</span><br><span class="line">1540</span><br><span class="line">1541</span><br><span class="line">1542</span><br><span class="line">1543</span><br><span class="line">1544</span><br><span class="line">1545</span><br><span class="line">1546</span><br><span class="line">1547</span><br><span class="line">1548</span><br><span class="line">1549</span><br><span class="line">1550</span><br><span class="line">1551</span><br><span class="line">1552</span><br><span class="line">1553</span><br><span class="line">1554</span><br><span class="line">1555</span><br><span class="line">1556</span><br><span class="line">1557</span><br><span class="line">1558</span><br><span class="line">1559</span><br><span class="line">1560</span><br><span class="line">1561</span><br><span class="line">1562</span><br><span class="line">1563</span><br><span class="line">1564</span><br><span class="line">1565</span><br><span class="line">1566</span><br><span class="line">1567</span><br><span class="line">1568</span><br><span class="line">1569</span><br><span class="line">1570</span><br><span class="line">1571</span><br><span class="line">1572</span><br><span class="line">1573</span><br><span class="line">1574</span><br><span class="line">1575</span><br><span class="line">1576</span><br><span class="line">1577</span><br><span class="line">1578</span><br><span class="line">1579</span><br><span class="line">1580</span><br><span class="line">1581</span><br><span class="line">1582</span><br><span class="line">1583</span><br><span class="line">1584</span><br><span class="line">1585</span><br><span class="line">1586</span><br><span class="line">1587</span><br><span class="line">1588</span><br><span class="line">1589</span><br><span class="line">1590</span><br><span class="line">1591</span><br><span class="line">1592</span><br><span class="line">1593</span><br><span class="line">1594</span><br><span class="line">1595</span><br><span class="line">1596</span><br><span class="line">1597</span><br><span class="line">1598</span><br><span class="line">1599</span><br><span class="line">1600</span><br><span class="line">1601</span><br><span class="line">1602</span><br><span class="line">1603</span><br><span class="line">1604</span><br><span class="line">1605</span><br><span class="line">1606</span><br><span class="line">1607</span><br><span class="line">1608</span><br><span class="line">1609</span><br><span class="line">1610</span><br><span class="line">1611</span><br><span class="line">1612</span><br><span class="line">1613</span><br><span class="line">1614</span><br><span class="line">1615</span><br><span class="line">1616</span><br><span class="line">1617</span><br><span class="line">1618</span><br><span class="line">1619</span><br><span class="line">1620</span><br><span class="line">1621</span><br><span class="line">1622</span><br><span class="line">1623</span><br><span class="line">1624</span><br><span class="line">1625</span><br><span class="line">1626</span><br><span class="line">1627</span><br><span class="line">1628</span><br><span class="line">1629</span><br><span class="line">1630</span><br><span class="line">1631</span><br><span class="line">1632</span><br><span class="line">1633</span><br><span class="line">1634</span><br><span class="line">1635</span><br><span class="line">1636</span><br><span class="line">1637</span><br><span class="line">1638</span><br><span class="line">1639</span><br><span class="line">1640</span><br><span class="line">1641</span><br><span class="line">1642</span><br><span class="line">1643</span><br><span class="line">1644</span><br><span class="line">1645</span><br><span class="line">1646</span><br><span class="line">1647</span><br><span class="line">1648</span><br><span class="line">1649</span><br><span class="line">1650</span><br><span class="line">1651</span><br><span class="line">1652</span><br><span class="line">1653</span><br><span class="line">1654</span><br><span class="line">1655</span><br><span class="line">1656</span><br><span class="line">1657</span><br><span class="line">1658</span><br><span class="line">1659</span><br><span class="line">1660</span><br><span class="line">1661</span><br><span class="line">1662</span><br><span class="line">1663</span><br><span class="line">1664</span><br><span class="line">1665</span><br><span class="line">1666</span><br><span class="line">1667</span><br><span class="line">1668</span><br><span class="line">1669</span><br><span class="line">1670</span><br><span class="line">1671</span><br><span class="line">1672</span><br><span class="line">1673</span><br><span class="line">1674</span><br><span class="line">1675</span><br><span class="line">1676</span><br><span class="line">1677</span><br><span class="line">1678</span><br><span class="line">1679</span><br><span class="line">1680</span><br><span class="line">1681</span><br><span class="line">1682</span><br><span class="line">1683</span><br><span class="line">1684</span><br><span class="line">1685</span><br><span class="line">1686</span><br><span class="line">1687</span><br><span class="line">1688</span><br><span class="line">1689</span><br><span class="line">1690</span><br><span class="line">1691</span><br><span class="line">1692</span><br><span class="line">1693</span><br><span class="line">1694</span><br><span class="line">1695</span><br><span class="line">1696</span><br><span class="line">1697</span><br><span class="line">1698</span><br><span class="line">1699</span><br><span class="line">1700</span><br><span class="line">1701</span><br><span class="line">1702</span><br><span class="line">1703</span><br><span class="line">1704</span><br><span class="line">1705</span><br><span class="line">1706</span><br><span class="line">1707</span><br><span class="line">1708</span><br><span class="line">1709</span><br><span class="line">1710</span><br><span class="line">1711</span><br><span class="line">1712</span><br><span class="line">1713</span><br><span class="line">1714</span><br><span class="line">1715</span><br><span class="line">1716</span><br><span class="line">1717</span><br><span class="line">1718</span><br><span class="line">1719</span><br><span class="line">1720</span><br><span class="line">1721</span><br><span class="line">1722</span><br><span class="line">1723</span><br><span class="line">1724</span><br><span class="line">1725</span><br><span class="line">1726</span><br><span class="line">1727</span><br><span class="line">1728</span><br><span class="line">1729</span><br><span class="line">1730</span><br><span class="line">1731</span><br><span class="line">1732</span><br><span class="line">1733</span><br><span class="line">1734</span><br><span class="line">1735</span><br><span class="line">1736</span><br><span class="line">1737</span><br><span class="line">1738</span><br><span class="line">1739</span><br><span class="line">1740</span><br><span class="line">1741</span><br><span class="line">1742</span><br><span class="line">1743</span><br><span class="line">1744</span><br><span class="line">1745</span><br><span class="line">1746</span><br><span class="line">1747</span><br><span class="line">1748</span><br><span class="line">1749</span><br><span class="line">1750</span><br><span class="line">1751</span><br><span class="line">1752</span><br><span class="line">1753</span><br><span class="line">1754</span><br><span class="line">1755</span><br><span class="line">1756</span><br><span class="line">1757</span><br><span class="line">1758</span><br><span class="line">1759</span><br><span class="line">1760</span><br><span class="line">1761</span><br><span class="line">1762</span><br><span class="line">1763</span><br><span class="line">1764</span><br><span class="line">1765</span><br><span class="line">1766</span><br><span class="line">1767</span><br><span class="line">1768</span><br><span class="line">1769</span><br><span class="line">1770</span><br><span class="line">1771</span><br><span class="line">1772</span><br><span class="line">1773</span><br><span class="line">1774</span><br><span class="line">1775</span><br><span class="line">1776</span><br><span class="line">1777</span><br><span class="line">1778</span><br><span class="line">1779</span><br><span class="line">1780</span><br><span class="line">1781</span><br><span class="line">1782</span><br><span class="line">1783</span><br><span class="line">1784</span><br><span class="line">1785</span><br><span class="line">1786</span><br><span class="line">1787</span><br><span class="line">1788</span><br><span class="line">1789</span><br><span class="line">1790</span><br><span class="line">1791</span><br><span class="line">1792</span><br><span class="line">1793</span><br><span class="line">1794</span><br><span class="line">1795</span><br><span class="line">1796</span><br><span class="line">1797</span><br><span class="line">1798</span><br><span class="line">1799</span><br><span class="line">1800</span><br><span class="line">1801</span><br><span class="line">1802</span><br><span class="line">1803</span><br><span class="line">1804</span><br><span class="line">1805</span><br><span class="line">1806</span><br><span class="line">1807</span><br><span class="line">1808</span><br><span class="line">1809</span><br><span class="line">1810</span><br><span class="line">1811</span><br><span class="line">1812</span><br><span class="line">1813</span><br><span class="line">1814</span><br><span class="line">1815</span><br><span class="line">1816</span><br><span class="line">1817</span><br><span class="line">1818</span><br><span class="line">1819</span><br><span class="line">1820</span><br><span class="line">1821</span><br><span class="line">1822</span><br><span class="line">1823</span><br><span class="line">1824</span><br><span class="line">1825</span><br><span class="line">1826</span><br><span class="line">1827</span><br><span class="line">1828</span><br><span class="line">1829</span><br><span class="line">1830</span><br><span class="line">1831</span><br><span class="line">1832</span><br><span class="line">1833</span><br><span class="line">1834</span><br><span class="line">1835</span><br><span class="line">1836</span><br><span class="line">1837</span><br><span class="line">1838</span><br><span class="line">1839</span><br><span class="line">1840</span><br><span class="line">1841</span><br><span class="line">1842</span><br><span class="line">1843</span><br><span class="line">1844</span><br><span class="line">1845</span><br><span class="line">1846</span><br><span class="line">1847</span><br><span class="line">1848</span><br><span class="line">1849</span><br><span class="line">1850</span><br><span class="line">1851</span><br><span class="line">1852</span><br><span class="line">1853</span><br><span class="line">1854</span><br><span class="line">1855</span><br><span class="line">1856</span><br><span class="line">1857</span><br><span class="line">1858</span><br><span class="line">1859</span><br><span class="line">1860</span><br><span class="line">1861</span><br><span class="line">1862</span><br><span class="line">1863</span><br><span class="line">1864</span><br><span class="line">1865</span><br><span class="line">1866</span><br><span class="line">1867</span><br><span class="line">1868</span><br><span class="line">1869</span><br><span class="line">1870</span><br><span class="line">1871</span><br><span class="line">1872</span><br><span class="line">1873</span><br><span class="line">1874</span><br><span class="line">1875</span><br><span class="line">1876</span><br><span class="line">1877</span><br><span class="line">1878</span><br><span class="line">1879</span><br><span class="line">1880</span><br><span class="line">1881</span><br><span class="line">1882</span><br><span class="line">1883</span><br><span class="line">1884</span><br><span class="line">1885</span><br><span class="line">1886</span><br><span class="line">1887</span><br><span class="line">1888</span><br><span class="line">1889</span><br><span class="line">1890</span><br><span class="line">1891</span><br><span class="line">1892</span><br><span class="line">1893</span><br><span class="line">1894</span><br><span class="line">1895</span><br><span class="line">1896</span><br><span class="line">1897</span><br><span class="line">1898</span><br><span class="line">1899</span><br><span class="line">1900</span><br><span class="line">1901</span><br><span class="line">1902</span><br><span class="line">1903</span><br><span class="line">1904</span><br><span class="line">1905</span><br><span class="line">1906</span><br><span class="line">1907</span><br><span class="line">1908</span><br><span class="line">1909</span><br><span class="line">1910</span><br><span class="line">1911</span><br><span class="line">1912</span><br><span class="line">1913</span><br><span class="line">1914</span><br><span class="line">1915</span><br><span class="line">1916</span><br><span class="line">1917</span><br><span class="line">1918</span><br><span class="line">1919</span><br><span class="line">1920</span><br><span class="line">1921</span><br><span class="line">1922</span><br><span class="line">1923</span><br><span class="line">1924</span><br><span class="line">1925</span><br><span class="line">1926</span><br><span class="line">1927</span><br><span class="line">1928</span><br><span class="line">1929</span><br><span class="line">1930</span><br><span class="line">1931</span><br><span class="line">1932</span><br><span class="line">1933</span><br><span class="line">1934</span><br><span class="line">1935</span><br><span class="line">1936</span><br><span class="line">1937</span><br><span class="line">1938</span><br><span class="line">1939</span><br><span class="line">1940</span><br><span class="line">1941</span><br><span class="line">1942</span><br><span class="line">1943</span><br><span class="line">1944</span><br><span class="line">1945</span><br><span class="line">1946</span><br><span class="line">1947</span><br><span class="line">1948</span><br><span class="line">1949</span><br><span class="line">1950</span><br><span class="line">1951</span><br><span class="line">1952</span><br><span class="line">1953</span><br><span class="line">1954</span><br><span class="line">1955</span><br><span class="line">1956</span><br><span class="line">1957</span><br><span class="line">1958</span><br><span class="line">1959</span><br><span class="line">1960</span><br><span class="line">1961</span><br><span class="line">1962</span><br><span class="line">1963</span><br><span class="line">1964</span><br><span class="line">1965</span><br><span class="line">1966</span><br><span class="line">1967</span><br><span class="line">1968</span><br><span class="line">1969</span><br><span class="line">1970</span><br><span class="line">1971</span><br><span class="line">1972</span><br><span class="line">1973</span><br><span class="line">1974</span><br><span class="line">1975</span><br><span class="line">1976</span><br><span class="line">1977</span><br><span class="line">1978</span><br><span class="line">1979</span><br><span class="line">1980</span><br><span class="line">1981</span><br><span class="line">1982</span><br><span class="line">1983</span><br><span class="line">1984</span><br><span class="line">1985</span><br><span class="line">1986</span><br><span class="line">1987</span><br><span class="line">1988</span><br><span class="line">1989</span><br><span class="line">1990</span><br><span class="line">1991</span><br><span class="line">1992</span><br><span class="line">1993</span><br><span class="line">1994</span><br><span class="line">1995</span><br><span class="line">1996</span><br><span class="line">1997</span><br><span class="line">1998</span><br><span class="line">1999</span><br><span class="line">2000</span><br><span class="line">2001</span><br><span class="line">2002</span><br><span class="line">2003</span><br><span class="line">2004</span><br><span class="line">2005</span><br><span class="line">2006</span><br><span class="line">2007</span><br><span class="line">2008</span><br><span class="line">2009</span><br><span class="line">2010</span><br><span class="line">2011</span><br><span class="line">2012</span><br><span class="line">2013</span><br><span class="line">2014</span><br><span class="line">2015</span><br><span class="line">2016</span><br><span class="line">2017</span><br><span class="line">2018</span><br><span class="line">2019</span><br><span class="line">2020</span><br><span class="line">2021</span><br><span class="line">2022</span><br><span class="line">2023</span><br><span class="line">2024</span><br><span class="line">2025</span><br><span class="line">2026</span><br><span class="line">2027</span><br><span class="line">2028</span><br><span class="line">2029</span><br><span class="line">2030</span><br><span class="line">2031</span><br><span class="line">2032</span><br><span class="line">2033</span><br><span class="line">2034</span><br><span class="line">2035</span><br><span class="line">2036</span><br><span class="line">2037</span><br><span class="line">2038</span><br><span class="line">2039</span><br><span class="line">2040</span><br><span class="line">2041</span><br><span class="line">2042</span><br><span class="line">2043</span><br><span class="line">2044</span><br><span class="line">2045</span><br><span class="line">2046</span><br><span class="line">2047</span><br><span class="line">2048</span><br><span class="line">2049</span><br><span class="line">2050</span><br><span class="line">2051</span><br><span class="line">2052</span><br><span class="line">2053</span><br><span class="line">2054</span><br><span class="line">2055</span><br><span class="line">2056</span><br><span class="line">2057</span><br><span class="line">2058</span><br><span class="line">2059</span><br><span class="line">2060</span><br><span class="line">2061</span><br><span class="line">2062</span><br><span class="line">2063</span><br><span class="line">2064</span><br><span class="line">2065</span><br><span class="line">2066</span><br><span class="line">2067</span><br><span class="line">2068</span><br><span class="line">2069</span><br><span class="line">2070</span><br><span class="line">2071</span><br><span class="line">2072</span><br><span class="line">2073</span><br><span class="line">2074</span><br><span class="line">2075</span><br><span class="line">2076</span><br><span class="line">2077</span><br><span class="line">2078</span><br><span class="line">2079</span><br><span class="line">2080</span><br><span class="line">2081</span><br><span class="line">2082</span><br><span class="line">2083</span><br><span class="line">2084</span><br><span class="line">2085</span><br><span class="line">2086</span><br><span class="line">2087</span><br><span class="line">2088</span><br><span class="line">2089</span><br><span class="line">2090</span><br><span class="line">2091</span><br><span class="line">2092</span><br><span class="line">2093</span><br><span class="line">2094</span><br><span class="line">2095</span><br><span class="line">2096</span><br><span class="line">2097</span><br><span class="line">2098</span><br><span class="line">2099</span><br><span class="line">2100</span><br><span class="line">2101</span><br><span class="line">2102</span><br><span class="line">2103</span><br><span class="line">2104</span><br><span class="line">2105</span><br><span class="line">2106</span><br><span class="line">2107</span><br><span class="line">2108</span><br><span class="line">2109</span><br><span class="line">2110</span><br><span class="line">2111</span><br><span class="line">2112</span><br><span class="line">2113</span><br><span class="line">2114</span><br><span class="line">2115</span><br><span class="line">2116</span><br><span class="line">2117</span><br><span class="line">2118</span><br><span class="line">2119</span><br><span class="line">2120</span><br><span class="line">2121</span><br><span class="line">2122</span><br><span class="line">2123</span><br><span class="line">2124</span><br><span class="line">2125</span><br><span class="line">2126</span><br><span class="line">2127</span><br><span class="line">2128</span><br><span class="line">2129</span><br><span class="line">2130</span><br><span class="line">2131</span><br><span class="line">2132</span><br><span class="line">2133</span><br><span class="line">2134</span><br><span class="line">2135</span><br><span class="line">2136</span><br><span class="line">2137</span><br><span class="line">2138</span><br><span class="line">2139</span><br><span class="line">2140</span><br><span class="line">2141</span><br><span class="line">2142</span><br><span class="line">2143</span><br><span class="line">2144</span><br><span class="line">2145</span><br><span class="line">2146</span><br><span class="line">2147</span><br><span class="line">2148</span><br><span class="line">2149</span><br><span class="line">2150</span><br><span class="line">2151</span><br><span class="line">2152</span><br><span class="line">2153</span><br><span class="line">2154</span><br><span class="line">2155</span><br><span class="line">2156</span><br><span class="line">2157</span><br><span class="line">2158</span><br><span class="line">2159</span><br><span class="line">2160</span><br><span class="line">2161</span><br><span class="line">2162</span><br><span class="line">2163</span><br><span class="line">2164</span><br><span class="line">2165</span><br><span class="line">2166</span><br><span class="line">2167</span><br><span class="line">2168</span><br><span class="line">2169</span><br><span class="line">2170</span><br><span class="line">2171</span><br><span class="line">2172</span><br><span class="line">2173</span><br><span class="line">2174</span><br><span class="line">2175</span><br><span class="line">2176</span><br><span class="line">2177</span><br><span class="line">2178</span><br><span class="line">2179</span><br><span class="line">2180</span><br><span class="line">2181</span><br><span class="line">2182</span><br><span class="line">2183</span><br><span class="line">2184</span><br><span class="line">2185</span><br><span class="line">2186</span><br><span class="line">2187</span><br><span class="line">2188</span><br><span class="line">2189</span><br><span class="line">2190</span><br><span class="line">2191</span><br><span class="line">2192</span><br><span class="line">2193</span><br><span class="line">2194</span><br><span class="line">2195</span><br><span class="line">2196</span><br><span class="line">2197</span><br><span class="line">2198</span><br><span class="line">2199</span><br><span class="line">2200</span><br><span class="line">2201</span><br><span class="line">2202</span><br><span class="line">2203</span><br><span class="line">2204</span><br><span class="line">2205</span><br><span class="line">2206</span><br><span class="line">2207</span><br><span class="line">2208</span><br><span class="line">2209</span><br><span class="line">2210</span><br><span class="line">2211</span><br><span class="line">2212</span><br><span class="line">2213</span><br><span class="line">2214</span><br><span class="line">2215</span><br><span class="line">2216</span><br><span class="line">2217</span><br><span class="line">2218</span><br><span class="line">2219</span><br><span class="line">2220</span><br><span class="line">2221</span><br><span class="line">2222</span><br><span class="line">2223</span><br><span class="line">2224</span><br><span class="line">2225</span><br><span class="line">2226</span><br><span class="line">2227</span><br><span class="line">2228</span><br><span class="line">2229</span><br><span class="line">2230</span><br><span class="line">2231</span><br><span class="line">2232</span><br><span class="line">2233</span><br><span class="line">2234</span><br><span class="line">2235</span><br><span class="line">2236</span><br><span class="line">2237</span><br><span class="line">2238</span><br><span class="line">2239</span><br><span class="line">2240</span><br><span class="line">2241</span><br><span class="line">2242</span><br><span class="line">2243</span><br><span class="line">2244</span><br><span class="line">2245</span><br><span class="line">2246</span><br><span class="line">2247</span><br><span class="line">2248</span><br><span class="line">2249</span><br><span class="line">2250</span><br><span class="line">2251</span><br><span class="line">2252</span><br><span class="line">2253</span><br><span class="line">2254</span><br><span class="line">2255</span><br><span class="line">2256</span><br><span class="line">2257</span><br><span class="line">2258</span><br><span class="line">2259</span><br><span class="line">2260</span><br><span class="line">2261</span><br><span class="line">2262</span><br><span class="line">2263</span><br><span class="line">2264</span><br></pre></td><td class="code"><pre><span class="line">model_vgg16.state_dict(): OrderedDict([(&#x27;features.0.weight&#x27;, tensor([[[[-0.0058, -0.0429,  0.0137],</span><br><span class="line">          [-0.0189,  0.0206,  0.1407],</span><br><span class="line">          [-0.0653,  0.0660, -0.0481]],</span><br><span class="line"></span><br><span class="line">         [[-0.0466,  0.0410,  0.0329],</span><br><span class="line">          [ 0.0776,  0.0679, -0.0501],</span><br><span class="line">          [ 0.0294, -0.0630,  0.0480]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0011,  0.0027,  0.0596],</span><br><span class="line">          [-0.0246,  0.0191,  0.0520],</span><br><span class="line">          [-0.0086, -0.0190,  0.0418]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.0062, -0.0272,  0.0264],</span><br><span class="line">          [ 0.0057, -0.0713, -0.0427],</span><br><span class="line">          [-0.0140, -0.0929, -0.0218]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0050,  0.0358,  0.1720],</span><br><span class="line">          [ 0.0286, -0.0034,  0.0096],</span><br><span class="line">          [-0.0026, -0.0059,  0.1130]],</span><br><span class="line"></span><br><span class="line">         [[-0.1007, -0.0538,  0.0306],</span><br><span class="line">          [-0.0770, -0.0982, -0.0755],</span><br><span class="line">          [-0.0319, -0.0327,  0.0189]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.1196, -0.0211,  0.0087],</span><br><span class="line">          [-0.0336, -0.0478, -0.1100],</span><br><span class="line">          [ 0.0894,  0.0243,  0.0103]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0776,  0.0885, -0.0192],</span><br><span class="line">          [ 0.0592,  0.0244,  0.0605],</span><br><span class="line">          [-0.0230, -0.0351, -0.1095]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0032, -0.0173, -0.0318],</span><br><span class="line">          [ 0.0171,  0.1167,  0.0647],</span><br><span class="line">          [ 0.0763,  0.0455, -0.0668]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.0720, -0.0316, -0.0387],</span><br><span class="line">          [ 0.0633, -0.0093, -0.0125],</span><br><span class="line">          [ 0.0289,  0.0909,  0.0148]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0046,  0.0179, -0.0522],</span><br><span class="line">          [ 0.0997, -0.0234,  0.0398],</span><br><span class="line">          [-0.0272, -0.0016,  0.0817]],</span><br><span class="line"></span><br><span class="line">         [[-0.0665, -0.1130,  0.1082],</span><br><span class="line">          [ 0.1044,  0.0397, -0.0058],</span><br><span class="line">          [-0.0478,  0.1348,  0.0747]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0733, -0.1851,  0.0318],</span><br><span class="line">          [-0.1138,  0.0740, -0.0374],</span><br><span class="line">          [ 0.0716,  0.1062, -0.0451]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0849, -0.0070, -0.0269],</span><br><span class="line">          [ 0.0577,  0.1075,  0.1082],</span><br><span class="line">          [ 0.0072, -0.0111,  0.0839]],</span><br><span class="line"></span><br><span class="line">         [[-0.0396, -0.0018,  0.0489],</span><br><span class="line">          [ 0.0084, -0.0689,  0.0476],</span><br><span class="line">          [ 0.0357,  0.1882, -0.0092]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0859, -0.0120, -0.0678],</span><br><span class="line">          [ 0.0339, -0.0270, -0.0450],</span><br><span class="line">          [ 0.0156,  0.0725,  0.0281]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0981,  0.0731,  0.0129],</span><br><span class="line">          [ 0.0324,  0.0689,  0.0429],</span><br><span class="line">          [-0.0592,  0.0655,  0.1102]],</span><br><span class="line"></span><br><span class="line">         [[-0.0312, -0.0004,  0.0006],</span><br><span class="line">          [-0.0115, -0.0425, -0.0801],</span><br><span class="line">          [-0.0376, -0.0284, -0.0141]]]])), (&#x27;features.0.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;features.2.weight&#x27;, tensor([[[[ 8.4797e-02,  2.8971e-02, -4.2445e-02],</span><br><span class="line">          [ 3.6968e-03, -3.5056e-02, -2.3158e-02],</span><br><span class="line">          [-8.0122e-02, -9.4988e-02, -5.4971e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.4578e-02,  1.3769e-01,  4.1991e-02],</span><br><span class="line">          [-9.4635e-02,  4.8270e-02, -4.3999e-02],</span><br><span class="line">          [ 3.2624e-03,  1.2303e-01,  4.9448e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 7.8258e-02, -6.8937e-02, -3.5214e-02],</span><br><span class="line">          [ 1.1342e-02, -5.2887e-02, -1.9984e-02],</span><br><span class="line">          [ 2.6641e-02,  6.1373e-02, -6.2112e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-4.6008e-02, -1.4316e-02, -8.6017e-02],</span><br><span class="line">          [ 9.2413e-02,  1.5329e-03, -1.1819e-02],</span><br><span class="line">          [-7.0963e-02, -1.3503e-03, -2.1069e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 4.7248e-02, -2.3619e-02,  6.1764e-02],</span><br><span class="line">          [-2.7669e-02, -5.1224e-02,  1.1275e-02],</span><br><span class="line">          [-2.0796e-02,  1.2217e-03, -6.2143e-02]],</span><br><span class="line"></span><br><span class="line">         [[-5.5395e-03,  9.0270e-02,  1.9917e-02],</span><br><span class="line">          [-5.9155e-02, -5.2784e-02,  6.7214e-02],</span><br><span class="line">          [-6.8221e-02, -1.6529e-01, -3.2909e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 9.4800e-02, -8.0121e-02, -1.2675e-02],</span><br><span class="line">          [-6.5118e-02,  6.7265e-02, -9.0730e-02],</span><br><span class="line">          [ 7.8983e-02,  3.9565e-02,  1.3089e-01]],</span><br><span class="line"></span><br><span class="line">         [[ 5.4955e-02,  3.5341e-02,  4.6366e-02],</span><br><span class="line">          [ 2.7913e-02, -4.4526e-02,  1.1365e-01],</span><br><span class="line">          [ 3.2010e-02,  2.1520e-02, -2.9198e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 5.4353e-02,  8.9082e-02,  8.9715e-03],</span><br><span class="line">          [ 9.1044e-03,  5.7604e-02,  2.2158e-02],</span><br><span class="line">          [ 1.0345e-01, -2.5009e-02, -6.7942e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-9.7426e-02,  9.2599e-03,  6.8386e-02],</span><br><span class="line">          [ 6.0771e-02, -4.3732e-03,  1.1483e-02],</span><br><span class="line">          [ 4.1002e-03,  8.9970e-02,  2.0537e-02]],</span><br><span class="line"></span><br><span class="line">         [[-4.6307e-02, -2.9111e-02, -4.2317e-02],</span><br><span class="line">          [-5.4095e-03,  3.3033e-02, -7.0233e-02],</span><br><span class="line">          [-2.1544e-02, -4.1733e-02,  8.3644e-02]],</span><br><span class="line"></span><br><span class="line">         [[-2.1626e-02, -4.4798e-02,  4.8739e-02],</span><br><span class="line">          [-3.9584e-02, -2.5416e-02, -4.5298e-02],</span><br><span class="line">          [-1.0902e-01, -1.9108e-02, -4.1865e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 4.0403e-02, -8.7034e-02, -7.5968e-02],</span><br><span class="line">          [-2.4019e-02,  2.2392e-02, -7.3035e-03],</span><br><span class="line">          [-3.2373e-04, -1.0101e-01, -5.8432e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.6826e-02, -2.5207e-02,  9.2228e-03],</span><br><span class="line">          [ 2.2508e-02,  5.6193e-02,  2.2061e-02],</span><br><span class="line">          [ 4.4507e-02, -1.8878e-02,  3.5220e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 9.5948e-02,  7.3020e-02, -7.7774e-04],</span><br><span class="line">          [ 5.3343e-03, -2.1242e-02, -4.4850e-02],</span><br><span class="line">          [ 1.7074e-02, -7.9259e-02, -7.1655e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 3.1196e-02,  2.1120e-02,  1.1530e-01],</span><br><span class="line">          [ 3.0199e-02, -2.5899e-03, -3.1567e-02],</span><br><span class="line">          [ 6.6479e-03,  1.3650e-02, -1.8059e-02]],</span><br><span class="line"></span><br><span class="line">         [[-3.3876e-02,  6.4873e-02, -6.6663e-02],</span><br><span class="line">          [ 4.3115e-02, -4.8905e-02,  4.3405e-02],</span><br><span class="line">          [-3.5761e-02,  3.5264e-02, -4.6147e-02]],</span><br><span class="line"></span><br><span class="line">         [[-3.1363e-02, -1.2756e-02, -3.9218e-02],</span><br><span class="line">          [-2.9347e-02,  2.9507e-02, -2.4010e-02],</span><br><span class="line">          [ 3.2064e-02,  2.9464e-02, -1.2343e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 2.6096e-03,  6.5317e-02,  5.2787e-02],</span><br><span class="line">          [-2.8571e-02, -5.8849e-02,  8.0178e-02],</span><br><span class="line">          [ 4.9149e-03, -3.5749e-02,  9.3951e-03]],</span><br><span class="line"></span><br><span class="line">         [[-1.2013e-01, -7.5116e-03,  7.4593e-02],</span><br><span class="line">          [ 5.0396e-02,  4.2715e-02, -1.1748e-02],</span><br><span class="line">          [ 4.5287e-02,  5.8500e-03, -1.3812e-01]],</span><br><span class="line"></span><br><span class="line">         [[ 8.2308e-02, -1.4029e-02, -9.5843e-03],</span><br><span class="line">          [-5.7437e-04, -6.8761e-02,  6.5826e-02],</span><br><span class="line">          [ 5.9781e-02, -2.2618e-02,  3.0288e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-6.2083e-02, -9.0287e-02,  1.8323e-02],</span><br><span class="line">          [ 2.6573e-02, -1.2315e-01, -2.0037e-02],</span><br><span class="line">          [-3.7915e-03, -6.8166e-03,  7.1848e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 9.6802e-02, -1.1498e-02, -6.8760e-02],</span><br><span class="line">          [-5.9423e-02, -3.7652e-02,  7.2002e-02],</span><br><span class="line">          [-3.4556e-05,  5.0035e-02, -6.5224e-02]],</span><br><span class="line"></span><br><span class="line">         [[-8.0316e-02, -9.9455e-03,  2.0094e-02],</span><br><span class="line">          [-9.8593e-02, -2.1017e-03, -6.9550e-02],</span><br><span class="line">          [-2.8378e-02,  6.8656e-02,  3.0612e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-7.1607e-02, -1.1480e-01, -4.3435e-02],</span><br><span class="line">          [ 1.8550e-02,  1.8030e-02,  6.0376e-02],</span><br><span class="line">          [ 3.6738e-02, -5.0909e-02,  4.0208e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.2145e-02, -3.6429e-03, -7.7804e-02],</span><br><span class="line">          [ 5.4205e-03, -6.0056e-02, -1.3281e-01],</span><br><span class="line">          [-3.7674e-02,  2.7231e-02,  2.7783e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 7.5123e-02,  5.2831e-02, -2.6043e-02],</span><br><span class="line">          [-8.1399e-03, -6.2251e-02,  4.5994e-02],</span><br><span class="line">          [ 6.3275e-02, -1.7219e-02, -7.2811e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 9.1590e-02, -3.9716e-02, -8.9942e-02],</span><br><span class="line">          [-1.3112e-03, -3.4848e-02, -7.1796e-02],</span><br><span class="line">          [ 3.4562e-02, -1.9008e-02, -2.8794e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.4581e-02,  5.6519e-03, -2.7933e-02],</span><br><span class="line">          [ 1.0518e-02, -1.6093e-01, -3.6386e-02],</span><br><span class="line">          [-4.3268e-02,  2.2478e-02,  1.0262e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.5004e-02,  3.7667e-02,  5.3874e-02],</span><br><span class="line">          [-9.1674e-03,  1.9764e-02, -2.8501e-02],</span><br><span class="line">          [-7.2572e-02,  4.8684e-02,  2.6788e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 5.1574e-03,  3.6132e-02, -6.5985e-02],</span><br><span class="line">          [ 3.4277e-02, -8.5457e-02, -1.3203e-01],</span><br><span class="line">          [-1.2226e-02,  3.3047e-02, -2.3952e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.5002e-01,  1.4479e-02, -3.5135e-02],</span><br><span class="line">          [-1.1939e-01, -2.4352e-02,  6.0766e-02],</span><br><span class="line">          [ 2.0641e-02,  3.7369e-02, -1.3175e-01]],</span><br><span class="line"></span><br><span class="line">         [[ 9.5272e-02,  9.7388e-03, -5.9880e-02],</span><br><span class="line">          [-1.7354e-02, -6.0511e-03,  3.8842e-02],</span><br><span class="line">          [-5.3795e-02, -1.1104e-01,  2.8688e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 3.4504e-02,  2.4141e-02,  6.7656e-02],</span><br><span class="line">          [-3.4801e-02,  5.3496e-02, -1.1584e-01],</span><br><span class="line">          [-3.7281e-02, -1.9649e-02, -1.3844e-01]],</span><br><span class="line"></span><br><span class="line">         [[ 9.6664e-03,  2.9376e-02, -8.6372e-02],</span><br><span class="line">          [ 3.0057e-02,  8.6891e-02,  1.1963e-02],</span><br><span class="line">          [ 3.2688e-02,  2.2282e-02, -3.8400e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 5.5859e-02,  8.7283e-02,  2.3035e-02],</span><br><span class="line">          [-5.0977e-02,  2.5675e-03, -7.1959e-02],</span><br><span class="line">          [ 5.9377e-02, -1.2214e-02, -4.4369e-02]]]])), (&#x27;features.2.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;features.5.weight&#x27;, tensor([[[[-0.0029, -0.0014, -0.0308],</span><br><span class="line">          [-0.0232, -0.0360, -0.0293],</span><br><span class="line">          [ 0.0162, -0.0139,  0.0085]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0912,  0.0142, -0.0535],</span><br><span class="line">          [ 0.0096,  0.0176, -0.0372],</span><br><span class="line">          [ 0.0087, -0.0794,  0.0235]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0289, -0.0384, -0.0229],</span><br><span class="line">          [-0.1000, -0.0459,  0.0201],</span><br><span class="line">          [-0.0714, -0.0613,  0.0197]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-0.0002, -0.0343, -0.0744],</span><br><span class="line">          [-0.0043, -0.0349,  0.0557],</span><br><span class="line">          [-0.0339,  0.0223, -0.0462]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0178,  0.0215,  0.0041],</span><br><span class="line">          [ 0.0106,  0.0307, -0.0767],</span><br><span class="line">          [ 0.0604, -0.0411,  0.0507]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0212, -0.0122,  0.0538],</span><br><span class="line">          [ 0.0072, -0.0267, -0.0350],</span><br><span class="line">          [ 0.0517, -0.0036, -0.0516]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0197,  0.0100, -0.0067],</span><br><span class="line">          [ 0.0701,  0.0611,  0.0866],</span><br><span class="line">          [ 0.0595,  0.0340, -0.0244]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0394, -0.0020, -0.0333],</span><br><span class="line">          [ 0.0786,  0.0216,  0.0428],</span><br><span class="line">          [-0.0823,  0.0546, -0.0116]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0515,  0.0389, -0.0634],</span><br><span class="line">          [ 0.0134, -0.0093,  0.0361],</span><br><span class="line">          [ 0.1121,  0.0018,  0.0461]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0030,  0.0421, -0.0280],</span><br><span class="line">          [-0.0073,  0.0310,  0.0089],</span><br><span class="line">          [ 0.0404,  0.0736, -0.0496]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0218,  0.0115, -0.0243],</span><br><span class="line">          [ 0.0046, -0.0298, -0.0698],</span><br><span class="line">          [-0.0538,  0.0334,  0.0180]],</span><br><span class="line"></span><br><span class="line">         [[-0.0431,  0.0337, -0.0278],</span><br><span class="line">          [ 0.0213,  0.0382, -0.0189],</span><br><span class="line">          [ 0.0232,  0.0075, -0.0188]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.0717,  0.0196, -0.0523],</span><br><span class="line">          [-0.0182,  0.0305,  0.0357],</span><br><span class="line">          [ 0.0990,  0.0991, -0.0561]],</span><br><span class="line"></span><br><span class="line">         [[-0.0006, -0.0440, -0.0381],</span><br><span class="line">          [-0.0286, -0.0024,  0.0686],</span><br><span class="line">          [ 0.0110,  0.0355, -0.0397]],</span><br><span class="line"></span><br><span class="line">         [[-0.0587,  0.0402, -0.0361],</span><br><span class="line">          [ 0.0347, -0.0476, -0.0351],</span><br><span class="line">          [-0.0744,  0.0690, -0.0590]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0587,  0.0095,  0.0723],</span><br><span class="line">          [ 0.0475, -0.0072,  0.0931],</span><br><span class="line">          [-0.0466, -0.0170,  0.0447]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0159, -0.0115,  0.0187],</span><br><span class="line">          [-0.0667,  0.0051, -0.0761],</span><br><span class="line">          [-0.0191,  0.0186,  0.0897]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0400, -0.0056, -0.0415],</span><br><span class="line">          [-0.0318,  0.0235,  0.0813],</span><br><span class="line">          [-0.0561,  0.0240, -0.0124]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.0407,  0.0236, -0.0226],</span><br><span class="line">          [ 0.0200, -0.0014,  0.0226],</span><br><span class="line">          [-0.0342,  0.0052,  0.0176]],</span><br><span class="line"></span><br><span class="line">         [[-0.0594, -0.0097, -0.0244],</span><br><span class="line">          [ 0.0249,  0.0523,  0.0775],</span><br><span class="line">          [ 0.0430,  0.0197, -0.0051]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0106, -0.0148,  0.0026],</span><br><span class="line">          [-0.0357,  0.0001, -0.1054],</span><br><span class="line">          [ 0.0414, -0.0192, -0.0608]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0421, -0.0577, -0.0339],</span><br><span class="line">          [ 0.0513, -0.0412,  0.0541],</span><br><span class="line">          [-0.0529, -0.0006, -0.0029]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0420, -0.0125, -0.0481],</span><br><span class="line">          [ 0.0280,  0.0414, -0.0316],</span><br><span class="line">          [ 0.0265, -0.0331, -0.0913]],</span><br><span class="line"></span><br><span class="line">         [[-0.0201,  0.0250,  0.0237],</span><br><span class="line">          [-0.0481, -0.0022, -0.0285],</span><br><span class="line">          [ 0.0716, -0.0570, -0.0386]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.0215,  0.1031,  0.0319],</span><br><span class="line">          [ 0.0060,  0.0012,  0.0447],</span><br><span class="line">          [-0.0242,  0.0180,  0.0293]],</span><br><span class="line"></span><br><span class="line">         [[-0.0227, -0.0222, -0.0434],</span><br><span class="line">          [-0.0378,  0.0658,  0.0025],</span><br><span class="line">          [-0.0026,  0.0402, -0.0678]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0019,  0.0224, -0.0389],</span><br><span class="line">          [-0.0243,  0.0220,  0.0709],</span><br><span class="line">          [-0.0342,  0.0076,  0.0170]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-0.0410, -0.0006,  0.0026],</span><br><span class="line">          [-0.0251, -0.0232,  0.0584],</span><br><span class="line">          [-0.0949, -0.0443, -0.0612]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0785, -0.0225, -0.0029],</span><br><span class="line">          [ 0.0168, -0.0173,  0.0545],</span><br><span class="line">          [-0.0113,  0.0879, -0.0625]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0196,  0.0724, -0.0410],</span><br><span class="line">          [-0.0149,  0.0175,  0.0248],</span><br><span class="line">          [ 0.0644, -0.0795,  0.0332]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0564, -0.0144, -0.0644],</span><br><span class="line">          [ 0.0041,  0.0207, -0.0162],</span><br><span class="line">          [ 0.0361, -0.1177,  0.0010]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0002, -0.0155,  0.0079],</span><br><span class="line">          [ 0.0347,  0.0016, -0.0377],</span><br><span class="line">          [ 0.0215, -0.0779, -0.0414]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0213,  0.0018,  0.0097],</span><br><span class="line">          [-0.0158,  0.0236,  0.0391],</span><br><span class="line">          [ 0.0027, -0.0044, -0.0640]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0951, -0.0306, -0.0369],</span><br><span class="line">          [-0.0931, -0.0206,  0.0223],</span><br><span class="line">          [-0.0428, -0.0052,  0.0048]],</span><br><span class="line"></span><br><span class="line">         [[-0.0176,  0.0841,  0.0361],</span><br><span class="line">          [ 0.0310,  0.0168,  0.0016],</span><br><span class="line">          [ 0.0064,  0.0478,  0.0015]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0081, -0.0341, -0.0182],</span><br><span class="line">          [-0.0086, -0.0008, -0.0028],</span><br><span class="line">          [ 0.0119, -0.0202, -0.0384]]]])), (&#x27;features.5.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;features.7.weight&#x27;, tensor([[[[ 1.6501e-02,  1.0378e-01,  8.9367e-03],</span><br><span class="line">          [ 1.6399e-02, -1.9415e-02,  2.2027e-02],</span><br><span class="line">          [ 5.4267e-02, -2.8150e-02, -2.6679e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 4.5888e-02,  2.1552e-02,  1.1759e-02],</span><br><span class="line">          [ 2.1807e-02,  4.6896e-02,  5.6470e-02],</span><br><span class="line">          [-3.3072e-02,  1.7963e-02,  1.0435e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.8164e-02, -3.8526e-02, -7.6670e-02],</span><br><span class="line">          [-1.0780e-02, -7.3000e-02, -1.5388e-04],</span><br><span class="line">          [-1.3709e-02,  6.0633e-02,  1.1000e-03]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 4.7984e-02, -2.6653e-03, -4.7430e-02],</span><br><span class="line">          [-1.1168e-01,  3.1873e-02, -2.1865e-02],</span><br><span class="line">          [-2.0273e-03,  6.0454e-02, -1.4240e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 2.5951e-02,  4.2385e-03, -7.8019e-03],</span><br><span class="line">          [-8.5244e-02,  4.0450e-02,  3.8535e-02],</span><br><span class="line">          [-6.8183e-03, -2.5962e-02, -3.4512e-03]],</span><br><span class="line"></span><br><span class="line">         [[-1.3681e-02, -7.1166e-03, -2.7988e-02],</span><br><span class="line">          [-2.2790e-02,  5.2191e-02, -3.1569e-02],</span><br><span class="line">          [-2.8774e-02,  1.1043e-02, -8.5053e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 5.2007e-02,  6.1991e-02,  4.7685e-02],</span><br><span class="line">          [ 7.9123e-04, -3.1703e-02,  4.2009e-02],</span><br><span class="line">          [ 3.4852e-02,  2.4266e-02, -5.8073e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.5957e-02, -5.7801e-02,  2.1571e-02],</span><br><span class="line">          [ 3.8188e-02,  9.2553e-03, -8.1309e-05],</span><br><span class="line">          [-5.0362e-02, -3.3008e-02, -3.5104e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 2.6691e-02, -2.5830e-02,  6.4328e-02],</span><br><span class="line">          [-8.2559e-04,  2.5616e-02, -6.6914e-02],</span><br><span class="line">          [-2.9020e-02, -5.7278e-02, -1.6361e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 4.5372e-02,  1.4441e-02,  3.9997e-02],</span><br><span class="line">          [-1.8190e-02, -3.3759e-02,  5.3796e-02],</span><br><span class="line">          [-2.0821e-02,  4.7421e-02,  4.3101e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 2.0568e-02, -4.8399e-02,  1.7584e-02],</span><br><span class="line">          [-3.1404e-02,  3.9553e-02, -1.6184e-02],</span><br><span class="line">          [-4.5007e-02,  4.3781e-02, -1.8262e-04]],</span><br><span class="line"></span><br><span class="line">         [[ 2.5786e-02,  8.2789e-03,  1.4926e-03],</span><br><span class="line">          [-3.3275e-02, -4.2372e-02,  1.0443e-02],</span><br><span class="line">          [ 5.4074e-02,  6.0261e-03,  3.5858e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 8.4032e-02, -1.5276e-02,  9.5789e-02],</span><br><span class="line">          [ 1.2047e-02, -1.8587e-02,  1.1287e-04],</span><br><span class="line">          [-6.7268e-02, -1.0312e-01,  3.6629e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.4008e-02,  1.9150e-02,  5.8700e-02],</span><br><span class="line">          [-8.1500e-02, -2.3769e-03, -6.6526e-02],</span><br><span class="line">          [ 4.1229e-02,  7.2496e-02, -4.0134e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 5.0155e-02, -2.1411e-02, -7.2541e-02],</span><br><span class="line">          [ 1.2146e-02,  1.5299e-02,  8.3484e-02],</span><br><span class="line">          [-9.9785e-02, -6.8519e-02, -6.1630e-03]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-3.2576e-02,  1.1641e-02, -4.4518e-02],</span><br><span class="line">          [ 1.4766e-02, -2.5786e-02,  7.9232e-02],</span><br><span class="line">          [-2.3472e-02,  4.4444e-02, -4.7550e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.5436e-02, -4.0958e-02,  2.4936e-02],</span><br><span class="line">          [-1.6222e-02,  7.5066e-02, -1.0550e-01],</span><br><span class="line">          [-3.6237e-02, -5.1774e-02,  1.6763e-03]],</span><br><span class="line"></span><br><span class="line">         [[-4.2939e-02,  1.9396e-02, -1.4527e-02],</span><br><span class="line">          [ 1.2674e-02, -3.4098e-02,  1.0925e-02],</span><br><span class="line">          [-9.1966e-04,  3.6910e-02,  6.9696e-03]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 2.0682e-02,  7.3224e-02, -2.4671e-02],</span><br><span class="line">          [-2.6790e-02, -4.7585e-03,  9.0610e-02],</span><br><span class="line">          [-2.2944e-02, -1.0345e-01,  6.3827e-02]],</span><br><span class="line"></span><br><span class="line">         [[-7.2391e-03, -3.0109e-02,  9.9681e-03],</span><br><span class="line">          [-1.0785e-02,  3.4157e-03,  7.7312e-02],</span><br><span class="line">          [ 5.1009e-02,  2.2542e-02, -2.0479e-02]],</span><br><span class="line"></span><br><span class="line">         [[-3.6194e-02, -3.0239e-02, -3.6729e-02],</span><br><span class="line">          [ 6.1556e-03, -1.7710e-02, -4.7740e-02],</span><br><span class="line">          [ 2.4981e-02, -8.3620e-02, -1.4947e-01]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 3.6845e-02, -1.0070e-02,  8.6513e-02],</span><br><span class="line">          [-1.3591e-03, -4.7372e-03,  1.6019e-02],</span><br><span class="line">          [-1.5788e-02,  2.1091e-02,  2.5630e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.5109e-02, -3.4237e-02,  4.0454e-02],</span><br><span class="line">          [ 1.3895e-02,  6.0834e-02,  6.8734e-02],</span><br><span class="line">          [ 2.0274e-02, -2.3780e-02, -6.3197e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 3.9001e-03, -3.6194e-02, -3.2106e-02],</span><br><span class="line">          [-1.5207e-02, -5.6638e-02,  2.6152e-02],</span><br><span class="line">          [ 5.5258e-02, -3.4312e-02,  1.0389e-01]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 1.1810e-02,  1.1419e-01,  1.4098e-02],</span><br><span class="line">          [ 4.7049e-02,  5.2147e-02,  2.3723e-02],</span><br><span class="line">          [ 1.0572e-02, -6.9217e-03, -2.0027e-02]],</span><br><span class="line"></span><br><span class="line">         [[-7.0163e-02, -5.9606e-02,  5.0043e-03],</span><br><span class="line">          [ 2.3094e-02,  7.3076e-03,  2.9700e-02],</span><br><span class="line">          [ 1.5212e-02,  1.3467e-02, -1.6010e-02]],</span><br><span class="line"></span><br><span class="line">         [[-5.1667e-02, -1.0641e-02, -2.5301e-02],</span><br><span class="line">          [ 8.0438e-03,  1.6526e-02, -2.7252e-02],</span><br><span class="line">          [ 7.1105e-02,  6.7926e-04, -6.2796e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 6.9677e-02,  3.6779e-02, -4.3647e-02],</span><br><span class="line">          [ 7.0757e-02,  7.2539e-02, -1.9451e-02],</span><br><span class="line">          [-6.6427e-02,  5.2805e-02,  4.7433e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.3786e-02, -3.2706e-02, -2.6781e-02],</span><br><span class="line">          [-5.7175e-02,  1.8031e-02,  2.4288e-02],</span><br><span class="line">          [ 6.8386e-03,  3.7457e-02,  2.7234e-02]],</span><br><span class="line"></span><br><span class="line">         [[-4.0257e-02, -5.0513e-02, -2.1028e-03],</span><br><span class="line">          [ 8.9102e-03,  2.8015e-02, -7.2539e-02],</span><br><span class="line">          [ 1.2797e-02,  5.8934e-03,  5.8229e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-3.6509e-03, -8.8202e-03, -3.3182e-02],</span><br><span class="line">          [-2.9320e-02, -2.8306e-02, -2.7669e-02],</span><br><span class="line">          [ 2.6412e-02,  1.9350e-02, -8.7551e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.2626e-02,  3.9181e-02,  6.9844e-02],</span><br><span class="line">          [ 7.5143e-02, -1.2007e-02, -5.2504e-02],</span><br><span class="line">          [-1.4397e-02,  6.2623e-03,  1.3975e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.6767e-02,  3.2615e-02,  9.7265e-02],</span><br><span class="line">          [-1.9832e-02,  9.1304e-04,  1.8536e-02],</span><br><span class="line">          [-4.9566e-03, -4.8595e-02, -2.6662e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-1.2798e-02,  7.1266e-02, -1.0997e-03],</span><br><span class="line">          [-9.7396e-03, -7.3191e-03, -1.4830e-02],</span><br><span class="line">          [ 1.5836e-02, -5.6168e-02,  1.3068e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.4069e-02,  4.6308e-02,  5.2846e-02],</span><br><span class="line">          [-6.7891e-03, -3.9499e-03, -1.1924e-02],</span><br><span class="line">          [ 4.6026e-02, -3.8070e-03, -1.9746e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.1951e-02,  3.5922e-02, -9.8934e-02],</span><br><span class="line">          [-6.8868e-02, -5.7159e-02, -3.5663e-02],</span><br><span class="line">          [ 6.0952e-03, -6.0121e-03,  3.1136e-02]]]])), (&#x27;features.7.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;features.10.weight&#x27;, tensor([[[[-0.0297,  0.0471, -0.0015],</span><br><span class="line">          [ 0.0415, -0.0011, -0.0204],</span><br><span class="line">          [-0.0430, -0.0357,  0.0225]],</span><br><span class="line"></span><br><span class="line">         [[-0.0337,  0.0689, -0.0313],</span><br><span class="line">          [ 0.0241, -0.0721, -0.0204],</span><br><span class="line">          [ 0.0017,  0.0084,  0.0105]],</span><br><span class="line"></span><br><span class="line">         [[-0.0096, -0.0079,  0.0153],</span><br><span class="line">          [-0.0282, -0.0283,  0.0119],</span><br><span class="line">          [ 0.0051, -0.0341,  0.0129]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0095, -0.0054, -0.0116],</span><br><span class="line">          [ 0.1052,  0.0374, -0.0181],</span><br><span class="line">          [ 0.0168, -0.0564, -0.0422]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0264,  0.0009, -0.0263],</span><br><span class="line">          [ 0.0173, -0.0539,  0.0096],</span><br><span class="line">          [-0.0153, -0.0081, -0.0069]],</span><br><span class="line"></span><br><span class="line">         [[-0.0074, -0.0208, -0.0162],</span><br><span class="line">          [ 0.0015, -0.0041,  0.0014],</span><br><span class="line">          [ 0.0265, -0.0139,  0.0226]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0458,  0.0004, -0.0074],</span><br><span class="line">          [ 0.0467, -0.0269, -0.0294],</span><br><span class="line">          [ 0.0017,  0.0160, -0.0368]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0507,  0.0132, -0.0435],</span><br><span class="line">          [-0.0348, -0.0221,  0.0230],</span><br><span class="line">          [ 0.0024,  0.0245, -0.0030]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0176,  0.0126, -0.0249],</span><br><span class="line">          [-0.0218,  0.0175,  0.0020],</span><br><span class="line">          [ 0.0109, -0.0504,  0.0175]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0262,  0.0265,  0.0448],</span><br><span class="line">          [ 0.0310,  0.0331,  0.0090],</span><br><span class="line">          [-0.0096, -0.0008, -0.0221]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0060,  0.0287,  0.0199],</span><br><span class="line">          [ 0.0194, -0.0229,  0.0088],</span><br><span class="line">          [ 0.0081,  0.0279, -0.0360]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0235,  0.0438,  0.0252],</span><br><span class="line">          [ 0.0222, -0.0209,  0.0162],</span><br><span class="line">          [ 0.0389,  0.0122, -0.0027]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.0247, -0.0353,  0.0083],</span><br><span class="line">          [-0.0366,  0.0449,  0.0082],</span><br><span class="line">          [ 0.0131,  0.0233,  0.0200]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0433,  0.0423, -0.0518],</span><br><span class="line">          [-0.0758,  0.0217, -0.0028],</span><br><span class="line">          [ 0.0080,  0.0197, -0.0268]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0069, -0.0068,  0.0054],</span><br><span class="line">          [ 0.0132, -0.0074,  0.0299],</span><br><span class="line">          [ 0.0671, -0.0238, -0.0065]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0120,  0.0414,  0.0070],</span><br><span class="line">          [ 0.0440,  0.0158,  0.0266],</span><br><span class="line">          [ 0.0077, -0.0415, -0.0448]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0166, -0.0160,  0.0035],</span><br><span class="line">          [ 0.0106,  0.0033,  0.0483],</span><br><span class="line">          [-0.0141, -0.0456,  0.0039]],</span><br><span class="line"></span><br><span class="line">         [[-0.0225, -0.0089, -0.0080],</span><br><span class="line">          [-0.0037,  0.0257, -0.0117],</span><br><span class="line">          [-0.0635,  0.0010,  0.0193]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0105,  0.0214, -0.0090],</span><br><span class="line">          [-0.0842,  0.0235,  0.0504],</span><br><span class="line">          [ 0.0416,  0.0225, -0.0110]],</span><br><span class="line"></span><br><span class="line">         [[-0.0261,  0.0059, -0.0135],</span><br><span class="line">          [-0.0367,  0.0084,  0.0003],</span><br><span class="line">          [-0.0157, -0.0281,  0.0294]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0321, -0.0113, -0.0001],</span><br><span class="line">          [-0.0233, -0.0311, -0.0107],</span><br><span class="line">          [ 0.0302,  0.0529, -0.0298]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0183, -0.0318, -0.0530],</span><br><span class="line">          [ 0.0197,  0.0195,  0.0063],</span><br><span class="line">          [-0.0455, -0.0250,  0.0083]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0151, -0.0298, -0.0429],</span><br><span class="line">          [-0.0663, -0.0040, -0.0494],</span><br><span class="line">          [ 0.0042,  0.0455, -0.0569]],</span><br><span class="line"></span><br><span class="line">         [[-0.0106, -0.0256,  0.0828],</span><br><span class="line">          [ 0.0075,  0.0199, -0.0146],</span><br><span class="line">          [-0.0605, -0.0028, -0.0371]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0160, -0.0299, -0.0261],</span><br><span class="line">          [-0.0105, -0.0031,  0.0035],</span><br><span class="line">          [ 0.0016, -0.0699, -0.0063]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0239,  0.0006, -0.0290],</span><br><span class="line">          [ 0.0463,  0.0188, -0.0535],</span><br><span class="line">          [ 0.0027, -0.0644,  0.0177]],</span><br><span class="line"></span><br><span class="line">         [[-0.0358, -0.0275,  0.0143],</span><br><span class="line">          [ 0.0134,  0.0075,  0.0168],</span><br><span class="line">          [ 0.0046, -0.0120, -0.0303]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0385,  0.0094,  0.0011],</span><br><span class="line">          [-0.0270, -0.0032, -0.0439],</span><br><span class="line">          [-0.0031,  0.0270,  0.0052]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0440,  0.0225, -0.0100],</span><br><span class="line">          [-0.0131, -0.0211,  0.0052],</span><br><span class="line">          [-0.0213,  0.0086,  0.0098]],</span><br><span class="line"></span><br><span class="line">         [[-0.0359,  0.0410, -0.0161],</span><br><span class="line">          [-0.0474, -0.0373,  0.0002],</span><br><span class="line">          [ 0.0212, -0.0227,  0.0148]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0116,  0.0035, -0.0252],</span><br><span class="line">          [-0.0040, -0.0132,  0.0055],</span><br><span class="line">          [ 0.0378,  0.0561,  0.0144]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0234, -0.0298,  0.0067],</span><br><span class="line">          [-0.0034, -0.0516, -0.0191],</span><br><span class="line">          [ 0.0005, -0.0320, -0.0278]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0087,  0.0195,  0.0161],</span><br><span class="line">          [ 0.0396,  0.0162, -0.0070],</span><br><span class="line">          [ 0.0082,  0.0019, -0.0037]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0598, -0.0402, -0.0368],</span><br><span class="line">          [ 0.0280,  0.0455, -0.0214],</span><br><span class="line">          [ 0.0779,  0.0338, -0.0363]],</span><br><span class="line"></span><br><span class="line">         [[-0.0216, -0.0437,  0.0266],</span><br><span class="line">          [ 0.0379, -0.0348,  0.0246],</span><br><span class="line">          [ 0.0022, -0.0165,  0.0350]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0346,  0.0257, -0.0381],</span><br><span class="line">          [ 0.0603, -0.0448,  0.0010],</span><br><span class="line">          [-0.0123,  0.0123,  0.0444]]]])), (&#x27;features.10.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;features.12.weight&#x27;, tensor([[[[-0.0487, -0.0085,  0.0612],</span><br><span class="line">          [-0.0376, -0.0006, -0.0213],</span><br><span class="line">          [-0.0043, -0.0263, -0.0053]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0023,  0.0178, -0.0267],</span><br><span class="line">          [ 0.0246,  0.0129,  0.0353],</span><br><span class="line">          [-0.0366, -0.0296, -0.0044]],</span><br><span class="line"></span><br><span class="line">         [[-0.0249,  0.0529,  0.0166],</span><br><span class="line">          [-0.0202,  0.0058,  0.0338],</span><br><span class="line">          [ 0.0123,  0.0236,  0.0606]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0008,  0.0095,  0.0381],</span><br><span class="line">          [-0.0122, -0.0182, -0.0189],</span><br><span class="line">          [ 0.0250, -0.0254,  0.0140]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0011,  0.0003, -0.0214],</span><br><span class="line">          [ 0.0075, -0.0347, -0.0265],</span><br><span class="line">          [ 0.0189,  0.0042, -0.0492]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0077, -0.0239,  0.0432],</span><br><span class="line">          [-0.0032,  0.0427, -0.0106],</span><br><span class="line">          [ 0.0183, -0.0654,  0.0191]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0006,  0.0260, -0.0027],</span><br><span class="line">          [ 0.0346,  0.0011,  0.0346],</span><br><span class="line">          [ 0.0054, -0.0043,  0.0387]],</span><br><span class="line"></span><br><span class="line">         [[-0.0107, -0.0177,  0.0325],</span><br><span class="line">          [ 0.0009,  0.0155, -0.0223],</span><br><span class="line">          [ 0.0029,  0.0232, -0.0427]],</span><br><span class="line"></span><br><span class="line">         [[-0.0070,  0.0682, -0.0198],</span><br><span class="line">          [ 0.0292, -0.0280, -0.0251],</span><br><span class="line">          [ 0.0101,  0.0260, -0.0138]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0193,  0.0133, -0.0040],</span><br><span class="line">          [ 0.0316,  0.0422, -0.0630],</span><br><span class="line">          [-0.0357,  0.0336, -0.0050]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0109,  0.0279, -0.0398],</span><br><span class="line">          [-0.0144,  0.0199, -0.0012],</span><br><span class="line">          [-0.0040, -0.0427, -0.0020]],</span><br><span class="line"></span><br><span class="line">         [[-0.0103, -0.0008,  0.0089],</span><br><span class="line">          [-0.0118,  0.0222,  0.0108],</span><br><span class="line">          [ 0.0074, -0.0060, -0.0078]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0266, -0.0219,  0.0742],</span><br><span class="line">          [-0.0138, -0.0043,  0.0249],</span><br><span class="line">          [-0.0197, -0.0546, -0.0530]],</span><br><span class="line"></span><br><span class="line">         [[-0.0070,  0.0028, -0.0566],</span><br><span class="line">          [ 0.0109,  0.0125,  0.0086],</span><br><span class="line">          [-0.0353, -0.0537, -0.0295]],</span><br><span class="line"></span><br><span class="line">         [[-0.0152, -0.0196, -0.0257],</span><br><span class="line">          [ 0.0449,  0.0235,  0.0286],</span><br><span class="line">          [ 0.0477,  0.0157,  0.0345]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-0.0089,  0.0390,  0.0243],</span><br><span class="line">          [-0.0075, -0.0218, -0.0264],</span><br><span class="line">          [-0.0093,  0.0162, -0.0017]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0325, -0.0017, -0.0074],</span><br><span class="line">          [-0.0046, -0.0143,  0.0224],</span><br><span class="line">          [-0.0187,  0.0060,  0.0071]],</span><br><span class="line"></span><br><span class="line">         [[-0.0072, -0.0014, -0.0328],</span><br><span class="line">          [-0.0228,  0.0055, -0.0018],</span><br><span class="line">          [ 0.0075, -0.0144,  0.0374]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0096, -0.0808, -0.0102],</span><br><span class="line">          [-0.0088,  0.0372,  0.0157],</span><br><span class="line">          [-0.0021, -0.0010, -0.0196]],</span><br><span class="line"></span><br><span class="line">         [[-0.0061,  0.0010,  0.0368],</span><br><span class="line">          [ 0.0138, -0.0680, -0.0508],</span><br><span class="line">          [-0.0667,  0.0011,  0.0091]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0094,  0.0049, -0.0119],</span><br><span class="line">          [ 0.0008,  0.0347,  0.0391],</span><br><span class="line">          [-0.0083,  0.0161,  0.0136]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-0.0378,  0.0009,  0.0732],</span><br><span class="line">          [-0.0176, -0.0316, -0.0001],</span><br><span class="line">          [-0.0055, -0.0285, -0.0028]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0132, -0.0143, -0.0235],</span><br><span class="line">          [ 0.0654,  0.0146, -0.0160],</span><br><span class="line">          [ 0.0048, -0.0137,  0.0322]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0037,  0.0160,  0.0066],</span><br><span class="line">          [-0.0281, -0.0313,  0.0375],</span><br><span class="line">          [ 0.0058, -0.0130, -0.0029]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.0177, -0.0112, -0.0570],</span><br><span class="line">          [-0.0198,  0.0343,  0.0261],</span><br><span class="line">          [-0.0424,  0.0018, -0.0060]],</span><br><span class="line"></span><br><span class="line">         [[-0.0433,  0.0155, -0.0057],</span><br><span class="line">          [ 0.0156,  0.0036, -0.0317],</span><br><span class="line">          [-0.0569, -0.0480,  0.0048]],</span><br><span class="line"></span><br><span class="line">         [[-0.0090, -0.0218, -0.0233],</span><br><span class="line">          [ 0.0255, -0.0165, -0.0466],</span><br><span class="line">          [-0.0268, -0.0034,  0.0090]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0039, -0.0202, -0.0192],</span><br><span class="line">          [ 0.0135,  0.0091,  0.0339],</span><br><span class="line">          [ 0.0052,  0.0120,  0.0115]],</span><br><span class="line"></span><br><span class="line">         [[-0.0449,  0.0375, -0.0544],</span><br><span class="line">          [ 0.0042,  0.0044, -0.0019],</span><br><span class="line">          [ 0.0162,  0.0151,  0.0579]],</span><br><span class="line"></span><br><span class="line">         [[-0.0003, -0.0205, -0.0004],</span><br><span class="line">          [ 0.0326, -0.0044, -0.0237],</span><br><span class="line">          [ 0.0232, -0.0213, -0.0031]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.0418,  0.0949, -0.0128],</span><br><span class="line">          [-0.0266,  0.0067, -0.0077],</span><br><span class="line">          [-0.0064, -0.0137,  0.0061]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0316, -0.0192,  0.0308],</span><br><span class="line">          [-0.0144, -0.0027,  0.0214],</span><br><span class="line">          [-0.0173,  0.0065,  0.0448]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0339, -0.0065, -0.0033],</span><br><span class="line">          [-0.0200,  0.0105,  0.0173],</span><br><span class="line">          [ 0.0347,  0.0215,  0.0241]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-0.0119, -0.0433, -0.0201],</span><br><span class="line">          [-0.0053, -0.0096, -0.0327],</span><br><span class="line">          [-0.0153, -0.0432,  0.0327]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0282,  0.0226, -0.0278],</span><br><span class="line">          [ 0.0026, -0.0354,  0.0098],</span><br><span class="line">          [ 0.0083, -0.0172,  0.0644]],</span><br><span class="line"></span><br><span class="line">         [[-0.0108, -0.0138, -0.0052],</span><br><span class="line">          [-0.0063,  0.0128, -0.0323],</span><br><span class="line">          [-0.0542,  0.0292, -0.0043]]]])), (&#x27;features.12.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;features.14.weight&#x27;, tensor([[[[-1.8905e-02,  4.7951e-02, -5.9256e-03],</span><br><span class="line">          [-4.9279e-03, -1.0121e-02, -1.2693e-02],</span><br><span class="line">          [-1.9263e-03,  1.1681e-02,  2.7548e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.9021e-02,  1.1109e-02,  8.4907e-04],</span><br><span class="line">          [ 3.2220e-02,  3.4402e-02,  3.6780e-03],</span><br><span class="line">          [-6.2312e-04,  4.5971e-03, -8.1376e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 5.7614e-02, -4.0676e-02, -1.4267e-02],</span><br><span class="line">          [ 8.5037e-03,  2.8689e-03,  2.2644e-02],</span><br><span class="line">          [-2.6688e-02, -2.1218e-03,  3.2260e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-1.2814e-02, -1.5844e-02,  4.4226e-03],</span><br><span class="line">          [-2.9731e-02,  2.2596e-02,  4.5153e-03],</span><br><span class="line">          [-6.8694e-02, -2.5545e-02, -1.3544e-04]],</span><br><span class="line"></span><br><span class="line">         [[ 3.6968e-02, -5.7705e-03, -5.7946e-02],</span><br><span class="line">          [-2.5901e-02,  1.8582e-02, -7.3952e-03],</span><br><span class="line">          [ 3.1773e-04,  6.5093e-02, -1.9809e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.4950e-02, -2.4824e-03,  2.5443e-02],</span><br><span class="line">          [ 8.5899e-03,  9.8378e-03, -9.6364e-03],</span><br><span class="line">          [ 2.2169e-02, -6.6588e-03,  5.6489e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 4.1860e-02, -2.7149e-03,  3.6731e-03],</span><br><span class="line">          [-7.8035e-03, -9.5097e-03, -6.7860e-02],</span><br><span class="line">          [ 3.8371e-02,  6.4870e-03, -1.7537e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 4.9351e-03,  1.7831e-02,  4.3551e-02],</span><br><span class="line">          [ 1.1696e-02, -2.0703e-02, -4.3055e-02],</span><br><span class="line">          [ 1.8140e-02,  3.9924e-02,  1.3187e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 4.6085e-02, -9.2465e-03, -9.9523e-03],</span><br><span class="line">          [ 2.3355e-02,  1.9439e-02, -6.1736e-02],</span><br><span class="line">          [ 1.3799e-02, -5.8381e-03,  5.9262e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 7.6792e-02, -1.7533e-02,  3.4787e-02],</span><br><span class="line">          [-1.9293e-02, -6.5327e-02,  6.8084e-03],</span><br><span class="line">          [ 8.0339e-03, -2.8802e-02, -1.8569e-03]],</span><br><span class="line"></span><br><span class="line">         [[-1.6403e-02,  1.6056e-02, -9.2139e-03],</span><br><span class="line">          [ 6.3721e-02, -7.8219e-03, -4.9037e-02],</span><br><span class="line">          [ 5.5994e-03,  4.0809e-02, -3.1391e-02]],</span><br><span class="line"></span><br><span class="line">         [[-2.9814e-02, -2.3161e-02, -1.0955e-02],</span><br><span class="line">          [-2.4525e-02,  2.5661e-02,  4.4539e-02],</span><br><span class="line">          [ 4.9945e-03,  2.6028e-02,  1.9386e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 1.1634e-02, -4.2903e-03, -4.2986e-02],</span><br><span class="line">          [ 5.6910e-02, -5.1090e-02, -4.7595e-03],</span><br><span class="line">          [ 1.2975e-02, -2.8365e-02,  8.4972e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 3.3031e-02,  4.7321e-02, -1.6843e-02],</span><br><span class="line">          [ 3.7167e-02,  1.8758e-02,  2.3908e-02],</span><br><span class="line">          [-4.6862e-03,  1.3742e-02,  2.7480e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.1431e-02,  2.3789e-02, -1.9687e-02],</span><br><span class="line">          [ 1.8256e-02,  2.5989e-02, -6.5714e-03],</span><br><span class="line">          [ 3.4420e-02,  1.1813e-02,  1.7318e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-1.8787e-02,  4.1937e-02, -4.5560e-04],</span><br><span class="line">          [-3.4615e-02, -2.9644e-02,  6.3797e-02],</span><br><span class="line">          [-4.3288e-02, -3.8939e-02, -4.3781e-02]],</span><br><span class="line"></span><br><span class="line">         [[-4.7952e-02,  2.8760e-02,  1.1230e-03],</span><br><span class="line">          [ 3.2180e-02, -1.5430e-02, -5.0261e-02],</span><br><span class="line">          [ 1.3211e-02, -3.6830e-03, -1.0297e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 2.8592e-03, -5.5839e-02,  6.3517e-04],</span><br><span class="line">          [-3.9500e-02,  1.2725e-02, -3.4035e-02],</span><br><span class="line">          [ 2.6453e-02, -9.7516e-03,  4.5713e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-3.7498e-02,  2.8881e-02, -2.6797e-02],</span><br><span class="line">          [ 3.2817e-02,  1.7581e-02,  3.8376e-03],</span><br><span class="line">          [ 1.1384e-02, -1.2536e-02, -3.7965e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.6562e-02,  3.7870e-02,  7.8580e-03],</span><br><span class="line">          [-7.3321e-03, -2.5192e-02, -4.0139e-02],</span><br><span class="line">          [-8.5199e-03, -7.3117e-05,  3.9342e-02]],</span><br><span class="line"></span><br><span class="line">         [[-4.5476e-02,  2.8176e-02, -1.9589e-02],</span><br><span class="line">          [-2.6867e-02,  1.5534e-02,  1.6755e-02],</span><br><span class="line">          [ 1.3545e-03,  3.3917e-02, -4.3265e-03]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 8.4092e-03,  1.0826e-02, -5.8868e-02],</span><br><span class="line">          [-2.3362e-03,  1.3827e-02, -2.0339e-02],</span><br><span class="line">          [-1.7023e-02,  2.7895e-02,  1.1939e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.0005e-02,  1.9215e-02, -7.6825e-02],</span><br><span class="line">          [-3.6907e-02,  3.6554e-02,  3.2598e-02],</span><br><span class="line">          [ 1.0690e-02,  1.9001e-02,  7.2939e-03]],</span><br><span class="line"></span><br><span class="line">         [[-1.2168e-02,  5.9378e-02,  2.0096e-02],</span><br><span class="line">          [ 9.3954e-04,  4.8637e-03,  1.5681e-02],</span><br><span class="line">          [-2.1155e-02, -3.1243e-02, -5.4531e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-3.2034e-02,  2.2083e-02,  8.4725e-03],</span><br><span class="line">          [ 7.9542e-03, -4.5646e-02,  4.4445e-03],</span><br><span class="line">          [ 3.9444e-02, -4.8159e-02, -5.0454e-02]],</span><br><span class="line"></span><br><span class="line">         [[-2.2254e-02, -6.0567e-03,  1.5396e-02],</span><br><span class="line">          [ 3.4244e-02,  2.2490e-02, -1.0069e-03],</span><br><span class="line">          [ 1.2218e-02,  3.7644e-02, -2.2574e-02]],</span><br><span class="line"></span><br><span class="line">         [[-7.0113e-02, -1.5091e-02,  9.4634e-03],</span><br><span class="line">          [ 7.3851e-03,  4.0033e-02,  5.7201e-03],</span><br><span class="line">          [-1.2774e-02, -1.9540e-04,  2.1566e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-2.1934e-02, -1.5675e-02,  7.9319e-03],</span><br><span class="line">          [ 3.1298e-02,  4.8693e-02, -3.4145e-02],</span><br><span class="line">          [ 3.3032e-03,  5.6892e-03,  2.8895e-02]],</span><br><span class="line"></span><br><span class="line">         [[-3.4726e-02,  5.0539e-02,  2.7997e-02],</span><br><span class="line">          [ 4.2760e-02, -1.2912e-02,  4.6157e-02],</span><br><span class="line">          [ 2.8956e-03,  3.6083e-02,  8.0212e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 5.3110e-03,  2.2642e-02, -3.0505e-02],</span><br><span class="line">          [ 1.0862e-02, -1.7523e-02, -1.7506e-02],</span><br><span class="line">          [-1.2165e-02, -5.8667e-02,  3.5321e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 2.3506e-02, -1.9071e-03,  3.7307e-03],</span><br><span class="line">          [-3.6492e-02,  4.0602e-02,  7.4822e-02],</span><br><span class="line">          [ 1.2425e-02,  1.5595e-02, -6.1345e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.7238e-02,  2.2839e-02, -4.3144e-02],</span><br><span class="line">          [-1.8985e-02, -2.1541e-02,  3.1021e-03],</span><br><span class="line">          [ 7.1597e-03, -9.1546e-04, -1.2228e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 5.0241e-02, -9.8962e-03,  2.8768e-03],</span><br><span class="line">          [-1.0629e-02,  7.6026e-02, -2.5636e-02],</span><br><span class="line">          [ 3.3283e-02,  2.0593e-02, -2.1422e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-4.0807e-03,  6.4193e-02, -3.2177e-02],</span><br><span class="line">          [-1.5444e-02,  3.3283e-02, -1.2394e-02],</span><br><span class="line">          [-2.2946e-04,  2.1308e-02, -2.5120e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 5.6895e-02,  2.6233e-02,  1.5276e-02],</span><br><span class="line">          [-1.7534e-02,  2.9851e-03, -2.0824e-02],</span><br><span class="line">          [ 4.1627e-02, -2.7531e-04,  1.1054e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 5.1693e-03, -3.5625e-02,  9.8889e-03],</span><br><span class="line">          [ 2.6093e-02, -1.0304e-02,  4.6907e-03],</span><br><span class="line">          [ 2.1263e-02,  8.3768e-03, -3.0365e-02]]]])), (&#x27;features.14.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;features.17.weight&#x27;, tensor([[[[ 1.2679e-02, -1.8388e-02,  1.0828e-02],</span><br><span class="line">          [ 8.4870e-03, -1.3784e-02, -2.9590e-03],</span><br><span class="line">          [-3.8458e-02, -1.5666e-02, -1.9581e-02]],</span><br><span class="line"></span><br><span class="line">         [[-2.5951e-02, -6.3973e-02,  4.6193e-02],</span><br><span class="line">          [-3.0161e-03,  4.5523e-03, -2.4834e-02],</span><br><span class="line">          [-1.6209e-02, -1.0580e-02, -1.2533e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 4.1337e-02,  1.8201e-04, -1.3593e-02],</span><br><span class="line">          [ 9.2874e-03,  1.1326e-02,  2.6692e-03],</span><br><span class="line">          [ 1.1298e-03, -2.5370e-04, -2.3583e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 8.1849e-03, -1.6214e-02, -2.3315e-02],</span><br><span class="line">          [ 3.0544e-03,  9.6189e-03,  4.0318e-03],</span><br><span class="line">          [ 3.2316e-02,  1.9915e-02, -1.4350e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 9.0660e-03,  2.1458e-02, -2.0900e-02],</span><br><span class="line">          [-3.5973e-02, -1.2452e-02,  2.0537e-02],</span><br><span class="line">          [-4.3425e-03,  9.2299e-03,  2.0557e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 1.8840e-02,  3.0326e-03, -9.5430e-03],</span><br><span class="line">          [-1.6705e-03,  1.3049e-02, -1.6494e-02],</span><br><span class="line">          [-8.2483e-03, -5.9907e-03, -4.8581e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-2.5143e-04, -5.0717e-03, -9.0269e-03],</span><br><span class="line">          [ 3.1059e-02,  2.5002e-02,  2.7854e-02],</span><br><span class="line">          [-7.3372e-03,  4.5101e-02, -6.8395e-03]],</span><br><span class="line"></span><br><span class="line">         [[-1.8434e-02,  2.1498e-02, -1.5749e-02],</span><br><span class="line">          [-1.6882e-03, -1.9380e-03, -3.7340e-02],</span><br><span class="line">          [-1.6465e-02, -1.0793e-02, -1.2197e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.7554e-02, -1.0657e-02,  1.0330e-02],</span><br><span class="line">          [-3.7711e-02, -8.8342e-03, -4.3821e-02],</span><br><span class="line">          [-2.1669e-02, -1.0322e-02,  6.7104e-03]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-7.2389e-03, -1.9203e-02, -1.8389e-02],</span><br><span class="line">          [-2.0089e-02, -2.3471e-02, -8.0331e-03],</span><br><span class="line">          [ 2.8603e-02, -7.4936e-03, -2.3379e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 6.0621e-04,  2.4512e-02, -3.0839e-02],</span><br><span class="line">          [-2.6096e-02,  3.5075e-02,  1.0888e-02],</span><br><span class="line">          [ 2.4236e-02, -2.3929e-03, -2.6149e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 9.6045e-03,  1.7005e-02,  2.5794e-02],</span><br><span class="line">          [ 3.3346e-02,  4.3666e-02, -9.8584e-03],</span><br><span class="line">          [-1.9569e-02,  3.1562e-02, -6.3773e-03]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 3.8934e-04,  1.3200e-02,  2.1915e-02],</span><br><span class="line">          [-3.2408e-05, -4.4664e-03, -1.7125e-02],</span><br><span class="line">          [-8.1567e-03,  1.4683e-02, -2.2453e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 4.3492e-03, -3.8470e-02, -8.3835e-03],</span><br><span class="line">          [ 1.5549e-02, -1.8317e-02, -2.2513e-02],</span><br><span class="line">          [-2.2624e-02, -2.3647e-02,  6.2665e-03]],</span><br><span class="line"></span><br><span class="line">         [[-3.6834e-02,  7.8556e-03,  3.8337e-03],</span><br><span class="line">          [-1.5317e-02,  5.2687e-03, -2.8403e-02],</span><br><span class="line">          [ 4.1019e-02, -1.8265e-03,  3.0696e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 3.7372e-02, -2.3195e-02, -3.6314e-02],</span><br><span class="line">          [-3.8160e-03,  8.3967e-03,  7.4484e-03],</span><br><span class="line">          [ 7.4891e-03, -1.5952e-02,  1.6208e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.8938e-02, -1.7215e-02,  3.1026e-03],</span><br><span class="line">          [ 2.8260e-02,  1.3350e-02, -2.3747e-03],</span><br><span class="line">          [ 3.2778e-02, -1.9980e-02, -1.7903e-02]],</span><br><span class="line"></span><br><span class="line">         [[-6.0901e-03, -9.4139e-03,  1.7843e-02],</span><br><span class="line">          [-2.9921e-03,  1.4008e-02, -3.0487e-02],</span><br><span class="line">          [-3.9325e-02,  1.9095e-02, -1.2105e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-2.2977e-03,  1.2732e-02, -4.5006e-03],</span><br><span class="line">          [-1.4452e-02, -8.3336e-03,  4.1299e-02],</span><br><span class="line">          [-1.4995e-02,  3.9340e-02, -2.1640e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.1876e-02, -1.2341e-02, -2.5863e-02],</span><br><span class="line">          [ 3.5768e-02, -1.5885e-02, -2.0903e-02],</span><br><span class="line">          [-2.5903e-02, -1.0950e-03,  3.8163e-03]],</span><br><span class="line"></span><br><span class="line">         [[-1.7407e-02,  2.1347e-02, -3.8038e-03],</span><br><span class="line">          [-1.0889e-02,  5.8342e-03,  2.9161e-02],</span><br><span class="line">          [-2.8307e-02, -4.3904e-02, -3.3995e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 2.7372e-02,  3.7014e-03, -2.4797e-02],</span><br><span class="line">          [-2.2302e-02,  2.3825e-02, -8.2529e-03],</span><br><span class="line">          [ 1.5101e-02, -1.3634e-02, -6.5303e-03]],</span><br><span class="line"></span><br><span class="line">         [[-2.6211e-02,  4.5676e-03,  1.5506e-02],</span><br><span class="line">          [-5.7283e-03,  1.1380e-02, -1.4154e-02],</span><br><span class="line">          [-1.3204e-02,  2.5766e-03, -7.1174e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 1.7746e-02,  8.5374e-03, -2.8894e-02],</span><br><span class="line">          [-2.6746e-03, -1.5999e-02, -1.7283e-03],</span><br><span class="line">          [-1.9270e-03, -5.1342e-03, -1.7360e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 1.4200e-02, -5.2498e-03, -9.7551e-03],</span><br><span class="line">          [ 1.5038e-02, -3.7479e-02, -3.3112e-02],</span><br><span class="line">          [ 2.5556e-03, -1.3806e-02,  3.5470e-02]],</span><br><span class="line"></span><br><span class="line">         [[-2.0233e-04,  2.9358e-02,  3.4747e-02],</span><br><span class="line">          [ 1.9121e-02, -1.6417e-02,  1.1285e-03],</span><br><span class="line">          [ 1.4031e-02,  3.4719e-02, -5.6685e-03]],</span><br><span class="line"></span><br><span class="line">         [[-4.8182e-03, -1.0469e-03,  6.0667e-03],</span><br><span class="line">          [ 1.4011e-02,  7.0741e-03, -4.5867e-02],</span><br><span class="line">          [ 2.3245e-02, -6.9671e-03,  1.0770e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 1.0514e-02, -1.2878e-02, -2.5052e-02],</span><br><span class="line">          [-5.3139e-03,  1.9816e-02, -2.1671e-02],</span><br><span class="line">          [-5.4537e-03,  1.2244e-02,  5.8666e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 6.4751e-03,  1.5380e-02,  2.9709e-02],</span><br><span class="line">          [-6.8313e-04,  1.4710e-05, -2.6962e-02],</span><br><span class="line">          [ 3.0019e-02, -3.9009e-03, -2.5287e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 8.0274e-03, -9.9244e-03,  9.4320e-03],</span><br><span class="line">          [ 2.8064e-03,  2.2174e-02, -6.3142e-03],</span><br><span class="line">          [-7.4501e-03,  5.6809e-02, -3.0940e-03]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-2.2182e-03, -1.2504e-02,  1.8376e-02],</span><br><span class="line">          [ 6.6300e-03,  3.1926e-03, -5.5482e-03],</span><br><span class="line">          [-7.5502e-03, -3.5590e-03,  2.1195e-02]],</span><br><span class="line"></span><br><span class="line">         [[-2.2975e-02,  1.1704e-02, -5.5784e-04],</span><br><span class="line">          [ 1.8214e-02, -9.5722e-03, -2.1152e-03],</span><br><span class="line">          [ 4.1358e-02, -2.8319e-03,  3.7769e-03]],</span><br><span class="line"></span><br><span class="line">         [[-9.0998e-03, -2.0431e-02, -1.7985e-03],</span><br><span class="line">          [-1.5559e-02,  4.4778e-02,  1.0857e-02],</span><br><span class="line">          [ 4.5258e-03,  2.3690e-02,  3.5134e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 2.9650e-03, -6.9018e-04,  5.1737e-03],</span><br><span class="line">          [-3.0738e-03,  4.5087e-03,  1.0360e-03],</span><br><span class="line">          [ 2.2299e-02, -3.3854e-03,  4.9482e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 6.7797e-03, -1.4997e-02, -2.1526e-02],</span><br><span class="line">          [ 2.8612e-02,  2.1746e-03, -2.6366e-02],</span><br><span class="line">          [ 3.1310e-03,  1.7821e-02, -3.0658e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.1876e-02, -1.1969e-02,  8.3984e-03],</span><br><span class="line">          [-1.8188e-02, -1.1248e-02,  7.7187e-04],</span><br><span class="line">          [ 2.1437e-02,  1.5949e-02, -1.1820e-03]]]])), (&#x27;features.17.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;features.19.weight&#x27;, tensor([[[[-0.0287,  0.0118, -0.0176],</span><br><span class="line">          [-0.0047, -0.0287,  0.0001],</span><br><span class="line">          [-0.0015, -0.0097,  0.0050]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0027, -0.0111, -0.0299],</span><br><span class="line">          [-0.0034,  0.0029, -0.0090],</span><br><span class="line">          [-0.0244,  0.0463, -0.0094]],</span><br><span class="line"></span><br><span class="line">         [[-0.0095, -0.0242,  0.0172],</span><br><span class="line">          [ 0.0044, -0.0441, -0.0294],</span><br><span class="line">          [ 0.0250,  0.0088, -0.0006]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0196, -0.0484, -0.0070],</span><br><span class="line">          [ 0.0095,  0.0378, -0.0235],</span><br><span class="line">          [ 0.0371,  0.0025, -0.0075]],</span><br><span class="line"></span><br><span class="line">         [[-0.0222,  0.0022, -0.0085],</span><br><span class="line">          [-0.0269, -0.0029,  0.0076],</span><br><span class="line">          [-0.0221, -0.0008, -0.0113]],</span><br><span class="line"></span><br><span class="line">         [[-0.0290,  0.0014, -0.0009],</span><br><span class="line">          [-0.0134,  0.0102,  0.0217],</span><br><span class="line">          [-0.0417, -0.0183, -0.0003]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.0042,  0.0049, -0.0463],</span><br><span class="line">          [ 0.0255, -0.0162, -0.0063],</span><br><span class="line">          [-0.0222,  0.0066, -0.0197]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0243, -0.0355,  0.0146],</span><br><span class="line">          [ 0.0144,  0.0046, -0.0103],</span><br><span class="line">          [-0.0082,  0.0200,  0.0191]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0246, -0.0078,  0.0024],</span><br><span class="line">          [ 0.0016,  0.0420, -0.0060],</span><br><span class="line">          [-0.0049,  0.0315, -0.0035]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0313, -0.0213, -0.0141],</span><br><span class="line">          [ 0.0055, -0.0085, -0.0159],</span><br><span class="line">          [ 0.0325,  0.0081,  0.0170]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0130, -0.0025,  0.0479],</span><br><span class="line">          [ 0.0230,  0.0196,  0.0141],</span><br><span class="line">          [-0.0077,  0.0202, -0.0183]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0006,  0.0099,  0.0014],</span><br><span class="line">          [-0.0362, -0.0040,  0.0133],</span><br><span class="line">          [ 0.0075, -0.0050, -0.0098]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0053, -0.0146, -0.0130],</span><br><span class="line">          [ 0.0091,  0.0261,  0.0118],</span><br><span class="line">          [-0.0147, -0.0077,  0.0289]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0158, -0.0047,  0.0016],</span><br><span class="line">          [-0.0162,  0.0061,  0.0059],</span><br><span class="line">          [ 0.0326, -0.0238,  0.0057]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0174, -0.0060,  0.0150],</span><br><span class="line">          [-0.0126, -0.0139,  0.0045],</span><br><span class="line">          [-0.0166,  0.0374,  0.0052]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-0.0381,  0.0190, -0.0502],</span><br><span class="line">          [-0.0035,  0.0101, -0.0186],</span><br><span class="line">          [ 0.0085,  0.0235, -0.0126]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0264, -0.0135, -0.0170],</span><br><span class="line">          [ 0.0355,  0.0005, -0.0318],</span><br><span class="line">          [ 0.0145, -0.0027,  0.0505]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0099,  0.0182, -0.0132],</span><br><span class="line">          [ 0.0535, -0.0167, -0.0209],</span><br><span class="line">          [ 0.0333,  0.0150, -0.0288]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0092, -0.0148, -0.0046],</span><br><span class="line">          [ 0.0223,  0.0333,  0.0194],</span><br><span class="line">          [-0.0122, -0.0111, -0.0067]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0292, -0.0109, -0.0259],</span><br><span class="line">          [-0.0166,  0.0094, -0.0196],</span><br><span class="line">          [-0.0094, -0.0124, -0.0184]],</span><br><span class="line"></span><br><span class="line">         [[-0.0146, -0.0056,  0.0531],</span><br><span class="line">          [ 0.0364,  0.0095,  0.0046],</span><br><span class="line">          [-0.0022, -0.0123, -0.0369]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0068,  0.0173,  0.0294],</span><br><span class="line">          [ 0.0018,  0.0116,  0.0160],</span><br><span class="line">          [-0.0205, -0.0271,  0.0450]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0009, -0.0075, -0.0112],</span><br><span class="line">          [ 0.0157, -0.0015,  0.0176],</span><br><span class="line">          [-0.0177, -0.0040, -0.0124]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0263,  0.0145, -0.0051],</span><br><span class="line">          [-0.0212,  0.0043, -0.0185],</span><br><span class="line">          [ 0.0045, -0.0034,  0.0127]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0097, -0.0060, -0.0352],</span><br><span class="line">          [ 0.0018,  0.0250,  0.0022],</span><br><span class="line">          [ 0.0191,  0.0395, -0.0145]],</span><br><span class="line"></span><br><span class="line">         [[-0.0251, -0.0025,  0.0315],</span><br><span class="line">          [ 0.0079, -0.0088,  0.0039],</span><br><span class="line">          [ 0.0389,  0.0279,  0.0011]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0329, -0.0082,  0.0004],</span><br><span class="line">          [ 0.0712, -0.0289,  0.0227],</span><br><span class="line">          [-0.0084,  0.0177,  0.0124]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-0.0029, -0.0422, -0.0087],</span><br><span class="line">          [-0.0058, -0.0162, -0.0241],</span><br><span class="line">          [ 0.0311, -0.0365, -0.0334]],</span><br><span class="line"></span><br><span class="line">         [[-0.0248, -0.0292,  0.0378],</span><br><span class="line">          [-0.0182,  0.0185, -0.0122],</span><br><span class="line">          [-0.0249,  0.0097,  0.0011]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0442, -0.0284, -0.0044],</span><br><span class="line">          [-0.0056, -0.0252, -0.0251],</span><br><span class="line">          [ 0.0275, -0.0014,  0.0053]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.0065,  0.0117,  0.0139],</span><br><span class="line">          [ 0.0432, -0.0038, -0.0098],</span><br><span class="line">          [ 0.0144, -0.0129,  0.0325]],</span><br><span class="line"></span><br><span class="line">         [[-0.0349, -0.0063, -0.0094],</span><br><span class="line">          [-0.0058,  0.0043, -0.0270],</span><br><span class="line">          [-0.0052, -0.0134, -0.0212]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0025,  0.0109, -0.0557],</span><br><span class="line">          [ 0.0189, -0.0009,  0.0008],</span><br><span class="line">          [-0.0040,  0.0301, -0.0001]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0311, -0.0258, -0.0302],</span><br><span class="line">          [-0.0118, -0.0081, -0.0049],</span><br><span class="line">          [-0.0015,  0.0127,  0.0093]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0161,  0.0278,  0.0042],</span><br><span class="line">          [-0.0121,  0.0014, -0.0179],</span><br><span class="line">          [-0.0144,  0.0146, -0.0032]],</span><br><span class="line"></span><br><span class="line">         [[-0.0069, -0.0498, -0.0048],</span><br><span class="line">          [ 0.0571, -0.0125,  0.0333],</span><br><span class="line">          [ 0.0418,  0.0381, -0.0009]]]])), (&#x27;features.19.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;features.21.weight&#x27;, tensor([[[[ 0.0073,  0.0257,  0.0282],</span><br><span class="line">          [-0.0283,  0.0167,  0.0053],</span><br><span class="line">          [ 0.0256,  0.0067,  0.0131]],</span><br><span class="line"></span><br><span class="line">         [[-0.0236,  0.0077,  0.0106],</span><br><span class="line">          [-0.0145, -0.0231, -0.0098],</span><br><span class="line">          [-0.0217, -0.0130, -0.0140]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0144, -0.0192,  0.0054],</span><br><span class="line">          [-0.0160,  0.0099,  0.0049],</span><br><span class="line">          [-0.0173, -0.0256,  0.0014]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-0.0002, -0.0429, -0.0002],</span><br><span class="line">          [-0.0288, -0.0024,  0.0028],</span><br><span class="line">          [ 0.0488, -0.0024, -0.0080]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0215, -0.0179, -0.0098],</span><br><span class="line">          [ 0.0192,  0.0114,  0.0198],</span><br><span class="line">          [ 0.0372,  0.0241, -0.0119]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0006, -0.0081, -0.0034],</span><br><span class="line">          [-0.0016, -0.0084, -0.0346],</span><br><span class="line">          [-0.0135, -0.0060, -0.0064]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0024,  0.0262,  0.0373],</span><br><span class="line">          [ 0.0161, -0.0114, -0.0022],</span><br><span class="line">          [-0.0122, -0.0254,  0.0429]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0092, -0.0585,  0.0214],</span><br><span class="line">          [ 0.0338,  0.0314,  0.0346],</span><br><span class="line">          [-0.0203, -0.0265,  0.0004]],</span><br><span class="line"></span><br><span class="line">         [[-0.0392,  0.0111, -0.0376],</span><br><span class="line">          [-0.0278, -0.0519, -0.0007],</span><br><span class="line">          [-0.0227, -0.0345,  0.0319]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-0.0191, -0.0352,  0.0217],</span><br><span class="line">          [-0.0281, -0.0020, -0.0253],</span><br><span class="line">          [ 0.0282, -0.0081, -0.0164]],</span><br><span class="line"></span><br><span class="line">         [[-0.0064,  0.0257, -0.0095],</span><br><span class="line">          [ 0.0104,  0.0043, -0.0376],</span><br><span class="line">          [ 0.0201, -0.0033, -0.0015]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0463, -0.0124,  0.0136],</span><br><span class="line">          [ 0.0002,  0.0288,  0.0122],</span><br><span class="line">          [-0.0277, -0.0215, -0.0072]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.0054, -0.0099,  0.0219],</span><br><span class="line">          [-0.0181, -0.0206,  0.0080],</span><br><span class="line">          [-0.0121, -0.0351, -0.0151]],</span><br><span class="line"></span><br><span class="line">         [[-0.0068,  0.0157, -0.0122],</span><br><span class="line">          [ 0.0125, -0.0254,  0.0189],</span><br><span class="line">          [-0.0278, -0.0076, -0.0214]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0067,  0.0095, -0.0015],</span><br><span class="line">          [-0.0124, -0.0158,  0.0136],</span><br><span class="line">          [ 0.0249, -0.0035,  0.0176]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0247, -0.0021, -0.0366],</span><br><span class="line">          [-0.0215,  0.0079,  0.0301],</span><br><span class="line">          [ 0.0302,  0.0006, -0.0545]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0145,  0.0184,  0.0311],</span><br><span class="line">          [-0.0075, -0.0204,  0.0024],</span><br><span class="line">          [ 0.0014, -0.0043,  0.0205]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0223,  0.0007, -0.0113],</span><br><span class="line">          [-0.0039,  0.0013,  0.0125],</span><br><span class="line">          [ 0.0107,  0.0224,  0.0082]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0062, -0.0319,  0.0110],</span><br><span class="line">          [ 0.0469, -0.0001,  0.0031],</span><br><span class="line">          [-0.0160,  0.0173, -0.0037]],</span><br><span class="line"></span><br><span class="line">         [[-0.0018,  0.0111, -0.0117],</span><br><span class="line">          [-0.0427,  0.0301, -0.0406],</span><br><span class="line">          [-0.0018, -0.0246,  0.0205]],</span><br><span class="line"></span><br><span class="line">         [[-0.0126, -0.0248,  0.0449],</span><br><span class="line">          [ 0.0061, -0.0020, -0.0061],</span><br><span class="line">          [ 0.0449,  0.0250, -0.0031]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0028,  0.0432, -0.0003],</span><br><span class="line">          [ 0.0037,  0.0292,  0.0067],</span><br><span class="line">          [-0.0076,  0.0094,  0.0122]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0137, -0.0165,  0.0366],</span><br><span class="line">          [-0.0228,  0.0271,  0.0128],</span><br><span class="line">          [ 0.0162, -0.0136,  0.0321]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0062,  0.0202,  0.0244],</span><br><span class="line">          [-0.0119, -0.0190,  0.0010],</span><br><span class="line">          [ 0.0341,  0.0010, -0.0060]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0235, -0.0123,  0.0149],</span><br><span class="line">          [ 0.0087, -0.0381, -0.0037],</span><br><span class="line">          [-0.0066, -0.0284,  0.0050]],</span><br><span class="line"></span><br><span class="line">         [[-0.0214,  0.0346,  0.0103],</span><br><span class="line">          [ 0.0211, -0.0254,  0.0249],</span><br><span class="line">          [-0.0057, -0.0044,  0.0073]],</span><br><span class="line"></span><br><span class="line">         [[-0.0063,  0.0184,  0.0317],</span><br><span class="line">          [-0.0117, -0.0010,  0.0139],</span><br><span class="line">          [ 0.0365, -0.0196,  0.0273]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0054,  0.0033,  0.0157],</span><br><span class="line">          [ 0.0045,  0.0360, -0.0449],</span><br><span class="line">          [ 0.0009, -0.0149,  0.0034]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0168, -0.0173, -0.0201],</span><br><span class="line">          [ 0.0160, -0.0091, -0.0106],</span><br><span class="line">          [-0.0038, -0.0142, -0.0078]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0228,  0.0072,  0.0331],</span><br><span class="line">          [ 0.0333, -0.0077,  0.0219],</span><br><span class="line">          [ 0.0078,  0.0019,  0.0272]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-0.0065, -0.0004,  0.0059],</span><br><span class="line">          [-0.0057, -0.0268, -0.0272],</span><br><span class="line">          [-0.0134,  0.0056, -0.0114]],</span><br><span class="line"></span><br><span class="line">         [[-0.0080, -0.0140, -0.0155],</span><br><span class="line">          [ 0.0196, -0.0248, -0.0208],</span><br><span class="line">          [ 0.0206,  0.0083,  0.0180]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0042, -0.0398, -0.0381],</span><br><span class="line">          [-0.0056,  0.0082, -0.0165],</span><br><span class="line">          [-0.0272,  0.0139,  0.0216]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 0.0179, -0.0710,  0.0226],</span><br><span class="line">          [-0.0077, -0.0476,  0.0133],</span><br><span class="line">          [ 0.0115,  0.0035,  0.0109]],</span><br><span class="line"></span><br><span class="line">         [[ 0.0137,  0.0014,  0.0130],</span><br><span class="line">          [ 0.0212,  0.0180,  0.0218],</span><br><span class="line">          [-0.0113,  0.0011, -0.0249]],</span><br><span class="line"></span><br><span class="line">         [[-0.0038, -0.0364,  0.0037],</span><br><span class="line">          [ 0.0123,  0.0071, -0.0160],</span><br><span class="line">          [-0.0082, -0.0088, -0.0315]]]])), (&#x27;features.21.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;features.24.weight&#x27;, tensor([[[[ 4.4139e-03, -1.1790e-02, -5.8919e-03],</span><br><span class="line">          [ 2.5536e-02,  2.3378e-02, -1.4968e-02],</span><br><span class="line">          [-2.6706e-03, -1.9048e-02,  1.4245e-02]],</span><br><span class="line"></span><br><span class="line">         [[-8.5705e-03, -3.9346e-02, -1.9782e-02],</span><br><span class="line">          [ 2.7233e-02,  2.6543e-02,  1.4646e-02],</span><br><span class="line">          [ 1.2960e-02,  9.2722e-03, -1.8193e-03]],</span><br><span class="line"></span><br><span class="line">         [[-9.9645e-03,  1.6960e-02, -8.0693e-03],</span><br><span class="line">          [-1.8161e-02,  3.5976e-03, -1.6803e-02],</span><br><span class="line">          [ 8.7888e-03, -3.0038e-02,  8.3387e-03]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 7.0128e-03, -3.2630e-02, -9.4621e-03],</span><br><span class="line">          [-3.1101e-02, -9.9281e-03, -4.7932e-03],</span><br><span class="line">          [-1.8536e-02,  1.4546e-04, -1.9674e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.3006e-02,  1.8718e-02, -5.7918e-02],</span><br><span class="line">          [ 2.9518e-02, -2.4184e-02,  1.4872e-02],</span><br><span class="line">          [ 2.4696e-02,  7.0396e-03,  5.3155e-03]],</span><br><span class="line"></span><br><span class="line">         [[-4.4328e-03,  2.1612e-02, -2.5334e-02],</span><br><span class="line">          [-4.3900e-03,  1.8172e-02, -1.6491e-02],</span><br><span class="line">          [-6.5093e-03, -3.6049e-02,  2.3232e-04]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 1.4065e-02,  5.7091e-03, -1.5178e-02],</span><br><span class="line">          [ 3.1930e-02, -2.0577e-05, -2.8929e-03],</span><br><span class="line">          [ 4.7086e-03,  2.7333e-02,  9.5228e-03]],</span><br><span class="line"></span><br><span class="line">         [[-1.7636e-02,  2.2691e-02, -3.6268e-03],</span><br><span class="line">          [-3.0758e-02, -1.1825e-02, -9.4367e-03],</span><br><span class="line">          [ 1.2725e-02, -4.0208e-03,  1.5523e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 4.1983e-02,  1.6394e-02, -8.4423e-03],</span><br><span class="line">          [ 2.0813e-02,  2.4824e-03, -1.0597e-02],</span><br><span class="line">          [ 7.7995e-03, -4.1313e-02,  6.5166e-04]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-1.1384e-02,  2.9245e-02, -1.2007e-02],</span><br><span class="line">          [-9.3143e-03, -4.5394e-02,  2.9730e-02],</span><br><span class="line">          [-1.2281e-02,  2.1481e-02, -9.3108e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 4.9849e-03, -1.9803e-03, -2.0778e-03],</span><br><span class="line">          [ 4.5571e-03,  1.1003e-02,  9.3186e-03],</span><br><span class="line">          [-4.8695e-03, -2.3445e-02, -1.9331e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 2.4593e-02, -2.3527e-02, -2.0184e-02],</span><br><span class="line">          [ 1.1399e-02,  3.1266e-03,  2.2745e-02],</span><br><span class="line">          [-5.4283e-03,  5.8386e-03, -4.0148e-03]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-5.0261e-02,  3.3362e-02, -9.4928e-03],</span><br><span class="line">          [ 6.7953e-03, -6.6075e-03,  1.5411e-02],</span><br><span class="line">          [-7.8562e-03,  7.8701e-03,  2.7145e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.3684e-02,  9.2703e-03, -8.6952e-03],</span><br><span class="line">          [ 2.3835e-02,  2.6430e-02, -1.6976e-02],</span><br><span class="line">          [ 2.4246e-02,  3.4682e-02,  1.1483e-02]],</span><br><span class="line"></span><br><span class="line">         [[-2.3648e-02,  2.9076e-02,  5.1161e-03],</span><br><span class="line">          [-1.2088e-02, -3.9844e-03,  1.6960e-02],</span><br><span class="line">          [-9.6439e-03, -1.0162e-02, -3.3090e-03]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-4.2847e-03, -2.6774e-02,  2.5058e-02],</span><br><span class="line">          [ 6.9562e-02,  1.4058e-02,  2.6681e-02],</span><br><span class="line">          [-6.7811e-03, -3.7118e-03, -1.7672e-02]],</span><br><span class="line"></span><br><span class="line">         [[-3.5642e-02, -4.9859e-03,  2.0640e-02],</span><br><span class="line">          [ 1.6194e-02,  2.6879e-03,  1.4762e-03],</span><br><span class="line">          [ 1.5981e-02, -8.9546e-03,  3.2181e-03]],</span><br><span class="line"></span><br><span class="line">         [[-6.2787e-03, -2.0935e-02,  5.7300e-03],</span><br><span class="line">          [ 4.6963e-03, -2.2517e-02,  1.1008e-02],</span><br><span class="line">          [-3.6225e-02, -2.2809e-02, -1.3054e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-5.5005e-03, -7.2045e-03, -2.9754e-02],</span><br><span class="line">          [ 2.2986e-03,  1.0520e-03,  3.7405e-03],</span><br><span class="line">          [ 2.2899e-02,  1.4394e-02,  1.2463e-02]],</span><br><span class="line"></span><br><span class="line">         [[-4.2083e-02,  1.5356e-02, -1.6664e-02],</span><br><span class="line">          [ 2.6349e-02, -8.8224e-03, -6.8574e-03],</span><br><span class="line">          [-4.2138e-03,  1.0117e-02,  6.1798e-03]],</span><br><span class="line"></span><br><span class="line">         [[-2.3459e-02, -1.5674e-02, -5.0234e-02],</span><br><span class="line">          [-5.5747e-03, -1.1873e-02, -2.2994e-02],</span><br><span class="line">          [ 1.9651e-02,  3.2847e-02, -7.1510e-04]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-1.5616e-02, -1.1973e-02, -4.7994e-03],</span><br><span class="line">          [ 7.6617e-04, -2.6854e-02, -1.6452e-02],</span><br><span class="line">          [-3.2775e-02, -6.4185e-03,  2.5588e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 6.4883e-03,  7.5846e-03, -3.6873e-02],</span><br><span class="line">          [-6.8940e-03, -3.1724e-03,  3.3657e-03],</span><br><span class="line">          [-2.9837e-03,  1.1895e-03,  3.6600e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.7726e-02,  1.4868e-02,  9.4435e-03],</span><br><span class="line">          [-1.5301e-02,  1.0535e-02, -1.2606e-03],</span><br><span class="line">          [ 1.9098e-02, -2.5352e-02,  2.7809e-03]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-1.5372e-02, -3.4387e-02, -7.8097e-03],</span><br><span class="line">          [-2.9510e-03, -3.6767e-02,  3.5891e-02],</span><br><span class="line">          [ 2.0748e-03,  2.5089e-02, -4.8764e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 3.6387e-02, -1.1795e-03, -1.0660e-02],</span><br><span class="line">          [-1.2834e-02, -2.2483e-02, -4.4173e-03],</span><br><span class="line">          [-2.4298e-02,  1.8748e-02, -1.1211e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.5142e-02, -3.4627e-02,  1.1231e-02],</span><br><span class="line">          [-5.7168e-03, -5.6466e-03, -1.2747e-02],</span><br><span class="line">          [-2.8890e-02,  3.9157e-02, -2.6380e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 7.7057e-03,  1.0486e-02,  2.6485e-02],</span><br><span class="line">          [ 2.9101e-02, -1.4863e-02,  1.0123e-02],</span><br><span class="line">          [ 2.6184e-02, -1.9687e-02, -2.0642e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.5983e-04,  3.4014e-02,  2.1482e-02],</span><br><span class="line">          [ 4.6727e-02,  1.5941e-02,  2.3560e-02],</span><br><span class="line">          [-1.8712e-02,  2.9075e-02,  9.9470e-03]],</span><br><span class="line"></span><br><span class="line">         [[-3.7601e-02,  3.0248e-03,  1.7833e-02],</span><br><span class="line">          [ 2.6566e-02, -2.9189e-02,  5.2624e-03],</span><br><span class="line">          [-7.1660e-03,  2.3804e-02, -3.4850e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 5.4724e-03, -1.4320e-02, -1.9203e-03],</span><br><span class="line">          [ 2.7736e-02, -2.2328e-02, -1.8567e-02],</span><br><span class="line">          [ 3.0091e-02, -3.6964e-03, -6.0846e-03]],</span><br><span class="line"></span><br><span class="line">         [[-1.9638e-02,  2.7072e-02, -2.7806e-03],</span><br><span class="line">          [-2.0427e-02,  1.4472e-03,  5.5313e-04],</span><br><span class="line">          [-2.1259e-02, -9.1961e-03,  2.1305e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 8.0042e-03,  2.6778e-02, -1.6323e-02],</span><br><span class="line">          [ 3.5089e-04, -4.9647e-02, -2.2565e-02],</span><br><span class="line">          [ 1.7065e-02,  1.4680e-03, -1.4234e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-4.3329e-02, -5.3790e-03,  4.5731e-03],</span><br><span class="line">          [ 4.1723e-03,  1.0477e-02, -2.6377e-03],</span><br><span class="line">          [ 9.9389e-03,  1.6898e-02,  7.0590e-04]],</span><br><span class="line"></span><br><span class="line">         [[-7.9182e-03,  5.7460e-03, -2.1806e-02],</span><br><span class="line">          [-4.2489e-02,  2.7433e-02, -2.3667e-03],</span><br><span class="line">          [ 1.4859e-02,  1.9317e-03,  2.7321e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.0082e-02,  2.3122e-02, -7.2049e-03],</span><br><span class="line">          [-2.4658e-02,  5.7622e-03, -5.8253e-03],</span><br><span class="line">          [ 1.5331e-03,  5.4762e-03, -2.8798e-02]]]])), (&#x27;features.24.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;features.26.weight&#x27;, tensor([[[[-1.3595e-02,  4.7354e-03, -1.0871e-03],</span><br><span class="line">          [ 2.4058e-03, -2.1290e-02,  8.1509e-03],</span><br><span class="line">          [-1.0648e-02,  1.5081e-02,  2.4422e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.9965e-02, -1.8593e-02, -7.5732e-03],</span><br><span class="line">          [-2.0483e-02, -3.8945e-03, -1.4306e-02],</span><br><span class="line">          [ 3.6904e-02, -4.1743e-02,  2.0924e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.3488e-02,  1.3473e-02, -4.6620e-02],</span><br><span class="line">          [-2.3061e-03,  9.5969e-03, -1.1231e-02],</span><br><span class="line">          [-1.8010e-02, -2.9332e-02, -7.7639e-03]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 1.4175e-02,  1.6211e-02, -4.4055e-02],</span><br><span class="line">          [ 7.1981e-04,  3.3908e-02,  3.9725e-02],</span><br><span class="line">          [-6.8925e-03,  3.1018e-03, -1.2288e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.6515e-02, -1.8472e-03, -6.3128e-03],</span><br><span class="line">          [-2.0364e-02, -7.9743e-03, -3.0309e-02],</span><br><span class="line">          [-4.7667e-02,  1.3344e-03,  3.4805e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.1800e-02, -9.4330e-03,  2.7623e-02],</span><br><span class="line">          [-1.7184e-02, -3.2891e-03,  1.3672e-02],</span><br><span class="line">          [ 1.6366e-02, -3.8358e-02,  2.9241e-03]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-3.5555e-03, -9.7490e-03, -2.4482e-02],</span><br><span class="line">          [ 2.2314e-03, -2.1646e-03, -3.6683e-02],</span><br><span class="line">          [-2.0806e-03, -3.9109e-03, -7.3461e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 1.4608e-03, -1.7872e-02,  3.4338e-03],</span><br><span class="line">          [-1.3528e-02,  3.3950e-03,  2.2069e-02],</span><br><span class="line">          [-1.1404e-02, -1.1412e-02, -1.7363e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 9.3509e-03, -8.9302e-03,  2.7042e-02],</span><br><span class="line">          [-4.3818e-03,  1.0560e-02,  1.2987e-03],</span><br><span class="line">          [-9.9823e-03,  2.7071e-03, -4.1122e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 2.6163e-03,  2.0012e-02, -2.5999e-03],</span><br><span class="line">          [-2.4896e-02,  2.9378e-02, -4.2108e-02],</span><br><span class="line">          [ 1.5615e-02,  1.6807e-02,  1.9322e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 2.1709e-02, -9.9444e-03,  1.2148e-02],</span><br><span class="line">          [ 2.4841e-02,  6.0691e-03,  1.2206e-02],</span><br><span class="line">          [ 2.2703e-02, -5.9264e-03,  1.3925e-02]],</span><br><span class="line"></span><br><span class="line">         [[-5.2979e-03, -3.3101e-02, -2.5091e-02],</span><br><span class="line">          [-1.7063e-03, -1.5985e-02, -1.3688e-03],</span><br><span class="line">          [-3.7905e-03,  1.1592e-02,  2.2344e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 1.5434e-02,  1.8889e-02,  7.9745e-03],</span><br><span class="line">          [ 3.4772e-04,  5.5453e-03, -4.2217e-03],</span><br><span class="line">          [ 9.3234e-03,  2.8502e-02, -9.0193e-03]],</span><br><span class="line"></span><br><span class="line">         [[-4.1318e-03,  2.4217e-02,  2.9515e-03],</span><br><span class="line">          [-7.2971e-03, -1.8845e-02,  4.0271e-02],</span><br><span class="line">          [ 3.5004e-02, -1.6539e-02, -1.3520e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.1194e-02, -2.5105e-03, -2.9535e-02],</span><br><span class="line">          [ 8.6535e-04, -3.0442e-03, -5.5627e-03],</span><br><span class="line">          [-1.2033e-02,  2.2137e-02,  2.5243e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 3.9649e-02,  1.9545e-02,  2.9340e-02],</span><br><span class="line">          [ 3.8336e-03, -1.3360e-02, -1.3195e-03],</span><br><span class="line">          [-2.7007e-02,  1.1687e-02,  3.3056e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.2718e-02,  8.7218e-03, -8.9667e-03],</span><br><span class="line">          [ 1.7427e-02,  1.9069e-02, -2.4600e-02],</span><br><span class="line">          [ 1.0189e-02,  1.0911e-02, -4.6903e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 2.9432e-02, -1.3765e-02, -5.1282e-03],</span><br><span class="line">          [-1.9984e-02, -6.8462e-03,  7.4332e-03],</span><br><span class="line">          [-1.0773e-02,  1.7299e-02, -3.6239e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 4.9232e-02,  2.6641e-02, -3.7249e-02],</span><br><span class="line">          [ 4.7608e-03, -7.9190e-04,  3.6379e-03],</span><br><span class="line">          [ 3.1580e-02, -1.0480e-02, -1.4594e-02]],</span><br><span class="line"></span><br><span class="line">         [[-5.7172e-03,  2.0656e-02, -2.2807e-02],</span><br><span class="line">          [ 5.8865e-02,  9.0573e-03,  2.5673e-03],</span><br><span class="line">          [ 2.0823e-02, -1.4201e-02,  4.8916e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 1.2403e-02, -2.8216e-02, -1.9630e-02],</span><br><span class="line">          [ 3.8774e-02,  2.3884e-02, -1.0754e-02],</span><br><span class="line">          [ 1.1714e-02, -2.0429e-02, -1.6859e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-1.6150e-02,  1.0201e-02,  3.1992e-03],</span><br><span class="line">          [ 1.0470e-02,  2.5982e-02, -1.4617e-02],</span><br><span class="line">          [-9.4759e-03, -7.8804e-03, -1.0712e-02]],</span><br><span class="line"></span><br><span class="line">         [[-8.9926e-03,  2.3853e-02,  5.4626e-03],</span><br><span class="line">          [-3.7738e-03, -6.4484e-03,  1.5544e-02],</span><br><span class="line">          [ 1.0356e-02, -5.3259e-03, -1.2659e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 9.3043e-03, -2.7697e-02, -2.1313e-02],</span><br><span class="line">          [-1.6792e-02, -1.0323e-02, -3.1831e-02],</span><br><span class="line">          [-1.9380e-02, -5.7206e-03, -2.7183e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 1.7805e-02, -3.7700e-02, -1.7421e-02],</span><br><span class="line">          [-1.0219e-02, -2.3841e-02, -1.2394e-02],</span><br><span class="line">          [ 1.0025e-02,  4.4221e-03, -2.0620e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.9333e-03, -1.4203e-03, -1.5354e-03],</span><br><span class="line">          [-4.2562e-02,  6.5410e-04, -1.0999e-02],</span><br><span class="line">          [ 1.7518e-02, -6.9328e-03,  2.4165e-03]],</span><br><span class="line"></span><br><span class="line">         [[-3.2730e-02,  2.0834e-02, -2.1484e-02],</span><br><span class="line">          [ 2.2395e-02,  2.9646e-05,  4.3297e-03],</span><br><span class="line">          [-2.0939e-02, -2.3567e-03, -1.4395e-03]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 1.7700e-02, -3.3870e-02,  4.1829e-02],</span><br><span class="line">          [ 1.8181e-02,  3.0825e-02, -1.6417e-02],</span><br><span class="line">          [-3.1889e-02, -2.2853e-02,  8.5902e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 9.9513e-03,  9.3649e-03,  2.4220e-02],</span><br><span class="line">          [-4.8388e-02,  1.7983e-02, -4.1232e-03],</span><br><span class="line">          [-5.3359e-04, -3.4958e-03, -1.1072e-03]],</span><br><span class="line"></span><br><span class="line">         [[-7.3572e-03,  2.5589e-02,  5.8928e-02],</span><br><span class="line">          [ 7.2595e-03,  1.5097e-02,  8.5200e-03],</span><br><span class="line">          [-4.3093e-02, -4.2702e-02, -3.6303e-03]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 1.9680e-02,  5.9558e-03, -3.0871e-02],</span><br><span class="line">          [-4.1054e-02,  9.4873e-03, -3.9841e-03],</span><br><span class="line">          [ 3.0549e-03, -2.4588e-02,  1.7093e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.5950e-02,  3.8504e-02, -2.2173e-02],</span><br><span class="line">          [-1.4160e-02, -4.6591e-03,  6.4751e-03],</span><br><span class="line">          [ 1.0889e-02, -5.0451e-03, -1.3942e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 9.5828e-03, -5.9358e-03, -4.3187e-02],</span><br><span class="line">          [ 2.6451e-02,  2.0893e-02, -1.4110e-04],</span><br><span class="line">          [ 1.2348e-02, -9.0955e-03,  3.1468e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-1.1473e-02, -1.3872e-03, -9.4470e-03],</span><br><span class="line">          [ 2.9897e-04, -1.0811e-02, -1.2993e-02],</span><br><span class="line">          [ 1.6493e-02,  1.6041e-02, -1.1337e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.3606e-02,  2.6695e-02, -4.1907e-03],</span><br><span class="line">          [ 1.6099e-02,  6.1461e-03, -8.9331e-03],</span><br><span class="line">          [ 1.1087e-03,  2.3611e-02,  1.8712e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 7.3524e-03, -1.1357e-02, -3.0800e-03],</span><br><span class="line">          [-2.5091e-03,  8.5004e-03, -1.1661e-02],</span><br><span class="line">          [ 2.4411e-02, -3.8054e-02, -4.7073e-03]]]])), (&#x27;features.26.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;features.28.weight&#x27;, tensor([[[[ 4.7677e-02, -1.3230e-02,  8.1548e-03],</span><br><span class="line">          [-4.0352e-03,  1.0282e-02, -1.1553e-02],</span><br><span class="line">          [ 4.1054e-03, -2.6963e-03,  4.9709e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.1410e-02, -1.0908e-02, -6.3225e-03],</span><br><span class="line">          [-2.3036e-02,  7.8924e-03,  3.2619e-02],</span><br><span class="line">          [-3.1066e-03, -1.1354e-03,  1.2378e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.6013e-02,  2.2473e-02,  2.1987e-02],</span><br><span class="line">          [ 3.1898e-04,  2.8984e-02,  5.2172e-04],</span><br><span class="line">          [-1.1020e-02, -2.5219e-02,  3.2607e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 6.3913e-03,  1.4033e-02,  1.4383e-02],</span><br><span class="line">          [ 1.0086e-02, -3.1807e-02,  8.8377e-03],</span><br><span class="line">          [ 5.3031e-02, -3.4593e-02, -3.8908e-03]],</span><br><span class="line"></span><br><span class="line">         [[-3.5474e-02, -2.8141e-02,  1.2112e-03],</span><br><span class="line">          [ 8.1246e-03,  1.2957e-02, -1.4954e-02],</span><br><span class="line">          [-1.1236e-02,  3.0175e-02,  1.7308e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 1.8624e-02,  2.6223e-02,  1.4721e-02],</span><br><span class="line">          [-1.2343e-02,  2.1278e-02,  4.8199e-03],</span><br><span class="line">          [-1.8735e-02, -3.1860e-02, -8.4439e-03]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-1.2671e-02,  1.2143e-02, -6.4075e-03],</span><br><span class="line">          [-1.6412e-03,  7.2491e-03, -1.6929e-02],</span><br><span class="line">          [ 1.4169e-02, -3.5377e-02, -6.4200e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 2.3317e-02,  2.7362e-03,  1.4620e-02],</span><br><span class="line">          [ 1.8331e-02, -2.3135e-02, -1.8348e-02],</span><br><span class="line">          [-4.3674e-03,  6.0306e-03, -1.1699e-02]],</span><br><span class="line"></span><br><span class="line">         [[-4.5525e-04, -8.6345e-03,  2.0143e-04],</span><br><span class="line">          [ 1.7412e-02,  2.8279e-02, -3.8840e-02],</span><br><span class="line">          [-1.7187e-02,  2.9076e-02,  1.2897e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 3.9473e-03,  7.8987e-04, -1.4081e-02],</span><br><span class="line">          [ 2.1880e-02, -4.0299e-03,  2.0433e-02],</span><br><span class="line">          [-1.4559e-02, -2.4871e-02,  1.3585e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 2.0810e-02,  1.0999e-02, -8.1414e-03],</span><br><span class="line">          [-8.9681e-03,  2.3177e-02,  4.1862e-02],</span><br><span class="line">          [ 2.4910e-02, -1.8156e-02, -1.2740e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.7978e-02, -1.6478e-02, -1.7488e-03],</span><br><span class="line">          [ 1.4865e-02, -2.4394e-02, -3.1158e-02],</span><br><span class="line">          [ 9.5260e-03, -3.0758e-02, -2.1305e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 1.7811e-02,  1.9848e-03, -4.8545e-03],</span><br><span class="line">          [-1.7618e-02, -3.9652e-02,  3.3693e-02],</span><br><span class="line">          [ 2.2586e-03, -6.8487e-03,  2.3916e-02]],</span><br><span class="line"></span><br><span class="line">         [[-2.1910e-02,  6.2350e-03, -2.1238e-02],</span><br><span class="line">          [-6.8249e-03, -3.9368e-02,  2.8828e-02],</span><br><span class="line">          [ 3.4268e-02,  2.8090e-03,  2.2948e-02]],</span><br><span class="line"></span><br><span class="line">         [[-5.3779e-02, -3.1862e-02,  4.7030e-02],</span><br><span class="line">          [ 1.8758e-03, -1.0329e-02,  1.4635e-02],</span><br><span class="line">          [ 2.4078e-02, -3.1616e-02,  7.8137e-03]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-4.5932e-02, -6.8133e-03, -1.7797e-02],</span><br><span class="line">          [ 2.2860e-02, -4.5914e-03,  6.2440e-03],</span><br><span class="line">          [-3.1331e-02, -3.1233e-02,  1.3869e-02]],</span><br><span class="line"></span><br><span class="line">         [[-2.7371e-02,  5.9546e-03,  1.6451e-02],</span><br><span class="line">          [-1.4397e-02, -1.6585e-02,  1.1014e-02],</span><br><span class="line">          [-1.7672e-02,  2.0104e-02,  3.4521e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 2.9042e-02,  8.6480e-03,  3.2373e-02],</span><br><span class="line">          [-4.9724e-02, -2.8699e-02, -2.2607e-02],</span><br><span class="line">          [ 3.1209e-02,  7.7053e-03,  2.5630e-03]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 7.4125e-03, -1.7887e-02, -1.3045e-03],</span><br><span class="line">          [ 9.0274e-03, -3.3346e-02,  6.6509e-03],</span><br><span class="line">          [ 6.5677e-03, -3.5715e-02,  3.7248e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 4.2841e-02, -4.4612e-02, -3.3910e-03],</span><br><span class="line">          [-1.5835e-03, -1.1928e-02,  1.6730e-02],</span><br><span class="line">          [ 2.0188e-02,  1.1934e-02, -2.1247e-03]],</span><br><span class="line"></span><br><span class="line">         [[-1.5246e-02, -1.9605e-02, -9.4908e-03],</span><br><span class="line">          [-5.0231e-03,  2.9539e-02,  7.4371e-04],</span><br><span class="line">          [-7.0291e-03,  1.9794e-02,  3.1886e-03]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[ 9.4785e-03, -2.7030e-02, -5.5179e-03],</span><br><span class="line">          [ 7.0882e-03, -7.2558e-03,  4.1875e-02],</span><br><span class="line">          [-3.7692e-02, -1.0297e-02,  2.1864e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.7117e-03,  1.0928e-02,  1.8433e-02],</span><br><span class="line">          [-2.3541e-02,  2.4344e-02, -1.1303e-02],</span><br><span class="line">          [ 2.7861e-02,  2.1177e-02, -3.5979e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.0497e-02, -1.3015e-02, -1.6354e-02],</span><br><span class="line">          [ 1.5330e-02, -1.2112e-02,  1.0714e-02],</span><br><span class="line">          [-1.4592e-02,  3.0647e-02, -3.3966e-02]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-2.4310e-02,  4.7333e-02,  1.1107e-02],</span><br><span class="line">          [-1.6226e-02, -4.8789e-02,  1.7594e-02],</span><br><span class="line">          [ 1.4929e-03,  2.4847e-02,  2.9359e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 3.7583e-02, -3.2330e-02,  1.7395e-02],</span><br><span class="line">          [-1.8329e-04,  2.6056e-02, -7.6507e-03],</span><br><span class="line">          [-8.6336e-03,  3.9711e-03, -2.8567e-02]],</span><br><span class="line"></span><br><span class="line">         [[-4.2859e-02, -1.7793e-02,  2.5614e-02],</span><br><span class="line">          [ 1.0095e-02,  2.0351e-02,  1.1280e-02],</span><br><span class="line">          [-3.5378e-02, -3.9780e-02,  4.0476e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-6.8776e-03,  3.4930e-03,  1.0139e-02],</span><br><span class="line">          [ 2.0268e-02, -1.7010e-02, -2.9081e-03],</span><br><span class="line">          [ 6.2959e-03, -2.3386e-03,  1.9025e-02]],</span><br><span class="line"></span><br><span class="line">         [[-8.4614e-03,  2.9688e-02,  1.9660e-02],</span><br><span class="line">          [-1.0650e-02,  2.7689e-02, -1.2784e-02],</span><br><span class="line">          [ 1.9926e-02,  8.9240e-03,  4.3321e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 7.8540e-03, -3.1220e-02,  5.1344e-03],</span><br><span class="line">          [-4.8948e-04, -1.7988e-04, -2.8152e-02],</span><br><span class="line">          [ 2.9947e-02, -1.9744e-02, -3.9572e-03]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 7.0109e-03,  1.1147e-02, -9.3618e-03],</span><br><span class="line">          [-3.9373e-02,  1.7342e-02,  9.6219e-03],</span><br><span class="line">          [-8.5551e-03,  2.5282e-02,  6.0644e-02]],</span><br><span class="line"></span><br><span class="line">         [[-1.7084e-02, -1.3949e-02,  2.0746e-02],</span><br><span class="line">          [-9.2763e-03,  5.8794e-03,  1.4759e-02],</span><br><span class="line">          [-1.1985e-02, -1.4836e-02, -1.6946e-02]],</span><br><span class="line"></span><br><span class="line">         [[ 9.8732e-03, -2.3482e-02,  4.9252e-02],</span><br><span class="line">          [-4.2687e-02, -5.2094e-03, -2.0529e-02],</span><br><span class="line">          [-2.2038e-02, -1.1615e-03, -4.8120e-02]],</span><br><span class="line"></span><br><span class="line">         ...,</span><br><span class="line"></span><br><span class="line">         [[-3.9964e-02, -1.3851e-02,  1.8069e-02],</span><br><span class="line">          [-2.0191e-05, -9.6384e-03, -1.3346e-02],</span><br><span class="line">          [ 1.2436e-02, -8.7784e-03,  3.9046e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 2.2711e-02,  3.3432e-02, -1.8907e-02],</span><br><span class="line">          [ 4.0572e-03, -2.3100e-02,  1.0936e-02],</span><br><span class="line">          [-1.9602e-02,  8.3197e-03,  4.1581e-03]],</span><br><span class="line"></span><br><span class="line">         [[ 2.0331e-02, -1.1299e-02, -1.9595e-02],</span><br><span class="line">          [-4.7769e-03, -4.4373e-03,  2.2468e-02],</span><br><span class="line">          [ 3.6742e-03, -1.0798e-02, -2.3690e-02]]]])), (&#x27;features.28.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0.])), (&#x27;classifier.0.weight&#x27;, tensor([[ 8.7326e-03, -2.2454e-03,  3.9680e-05,  ...,  8.6021e-03,</span><br><span class="line">          8.6066e-03, -1.1813e-04],</span><br><span class="line">        [-1.7061e-02,  2.6901e-03, -2.2031e-03,  ...,  7.9405e-03,</span><br><span class="line">         -1.2853e-03, -4.1104e-03],</span><br><span class="line">        [-1.1634e-02, -8.3620e-03,  5.3148e-03,  ...,  1.1541e-02,</span><br><span class="line">         -2.6899e-03,  4.6875e-03],</span><br><span class="line">        ...,</span><br><span class="line">        [-7.0797e-03,  2.9927e-03, -2.0842e-02,  ..., -2.1298e-03,</span><br><span class="line">          7.6661e-03, -7.3799e-04],</span><br><span class="line">        [ 1.5296e-03,  1.7415e-02,  4.8609e-03,  ..., -5.8851e-03,</span><br><span class="line">          3.6787e-03,  4.7664e-04],</span><br><span class="line">        [ 1.3556e-02,  3.8480e-03, -4.9480e-03,  ..., -7.5195e-03,</span><br><span class="line">          1.6858e-02,  1.5710e-03]])), (&#x27;classifier.0.bias&#x27;, tensor([0., 0., 0.,  ..., 0., 0., 0.])), (&#x27;classifier.3.weight&#x27;, tensor([[-0.0028,  0.0042, -0.0102,  ...,  0.0115,  0.0079,  0.0099],</span><br><span class="line">        [-0.0118, -0.0157,  0.0081,  ..., -0.0061, -0.0019,  0.0206],</span><br><span class="line">        [ 0.0034,  0.0021, -0.0004,  ..., -0.0041,  0.0034,  0.0033],</span><br><span class="line">        ...,</span><br><span class="line">        [ 0.0141, -0.0004, -0.0161,  ...,  0.0141, -0.0144,  0.0017],</span><br><span class="line">        [ 0.0007, -0.0061, -0.0108,  ..., -0.0064,  0.0024,  0.0091],</span><br><span class="line">        [ 0.0135, -0.0150, -0.0102,  ..., -0.0085, -0.0137, -0.0056]])), (&#x27;classifier.3.bias&#x27;, tensor([0., 0., 0.,  ..., 0., 0., 0.])), (&#x27;classifier.6.weight&#x27;, tensor([[ 4.3585e-03, -7.9109e-03, -4.3370e-03,  ...,  4.7107e-03,</span><br><span class="line">          1.9101e-02,  3.0828e-03],</span><br><span class="line">        [-7.8912e-03,  6.3033e-03,  8.5372e-03,  ...,  3.2390e-04,</span><br><span class="line">          7.0038e-03, -2.0247e-03],</span><br><span class="line">        [-5.1779e-03, -8.0205e-03,  1.0307e-03,  ...,  1.1701e-02,</span><br><span class="line">          5.9549e-03,  2.2098e-02],</span><br><span class="line">        ...,</span><br><span class="line">        [-4.6863e-05, -1.5645e-02,  1.1493e-02,  ..., -1.1055e-02,</span><br><span class="line">          7.6899e-03, -4.5109e-03],</span><br><span class="line">        [-4.2128e-03, -1.8097e-02,  1.1364e-02,  ..., -1.9879e-02,</span><br><span class="line">          2.4911e-02,  2.7932e-03],</span><br><span class="line">        [-2.4500e-03,  2.7684e-03,  2.6685e-03,  ..., -6.4828e-03,</span><br><span class="line">         -2.4765e-04,  1.5845e-02]])), (&#x27;classifier.6.bias&#x27;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span><br><span class="line">        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))])</span><br></pre></td></tr></table></figure>
<p>模型参数的保存问题：</p>
<p>利用torch.model进行保存，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_3_4_1</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg3.4.1 : torch.save(model.state_dict(), f)</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line">  model_vgg16 = models.vgg16()</span><br><span class="line">  torch.save(model_vgg16.state_dict(), <span class="string">&quot;./vgg16.pth&quot;</span>,)</span><br><span class="line"></span><br><span class="line">eg_3_4_1()</span><br></pre></td></tr></table></figure>
<p>模型参数载入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_3_4_2</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg3.4.2 : model.load_state_dict()</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line">  model_vgg16 = models.vgg16()</span><br><span class="line">  state_dict = torch.load(<span class="string">&quot;./vgg16.pth&quot;</span>, map_location=<span class="string">&quot;cpu&quot;</span>)<span class="comment">#把vgg16文件载入要先读出数据torch.load</span></span><br><span class="line">  missing_keys, unexpected_keys = model_vgg16.load_state_dict(state_dict, strict=<span class="literal">True</span>)<span class="comment">#利用model里的方法载入模型</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;missing_keys: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(missing_keys))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;unexpected_keys: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(unexpected_keys))</span><br><span class="line"></span><br><span class="line">eg_3_4_2()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">missing_keys: []#空 代表参数都对的上 载入没有问题</span><br><span class="line">unexpected_keys: []</span><br></pre></td></tr></table></figure>
<p>下面我们把所有.bias参数都删了，再来试一次</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_3_4_3</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg3.4.3 : strict=False</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line">  model_vgg16 = models.vgg16()</span><br><span class="line">  state_dict = torch.load(<span class="string">&quot;./vgg16.pth&quot;</span>, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">  <span class="keyword">for</span> key <span class="keyword">in</span> <span class="built_in">list</span>(state_dict.keys()):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;.bias&quot;</span> <span class="keyword">in</span> key:</span><br><span class="line">      <span class="keyword">del</span> state_dict[key]</span><br><span class="line"></span><br><span class="line">  missing_keys, unexpected_keys = model_vgg16.load_state_dict(state_dict, strict=<span class="literal">False</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;missing_keys: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(missing_keys))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;unexpected_keys: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(unexpected_keys))</span><br><span class="line"></span><br><span class="line">eg_3_4_3()</span><br></pre></td></tr></table></figure>
<p>就会发现很多bias都不见了，并且如果strict=False改成True，那么会直接报错</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">missing_keys: [&#x27;features.0.bias&#x27;, &#x27;features.2.bias&#x27;, &#x27;features.5.bias&#x27;, &#x27;features.7.bias&#x27;, &#x27;features.10.bias&#x27;, &#x27;features.12.bias&#x27;, &#x27;features.14.bias&#x27;, &#x27;features.17.bias&#x27;, &#x27;features.19.bias&#x27;, &#x27;features.21.bias&#x27;, &#x27;features.24.bias&#x27;, &#x27;features.26.bias&#x27;, &#x27;features.28.bias&#x27;, &#x27;classifier.0.bias&#x27;, &#x27;classifier.3.bias&#x27;, &#x27;classifier.6.bias&#x27;]</span><br><span class="line">unexpected_keys: []</span><br></pre></td></tr></table></figure>
<h3 id="3-5-利用-torch-utils-model-zoo-load-url-下载预训练参数"><a href="#3-5-利用-torch-utils-model-zoo-load-url-下载预训练参数" class="headerlink" title="3.5 利用 torch.utils.model_zoo.load_url() 下载预训练参数"></a>3.5 利用 <code>torch.utils.model_zoo.load_url()</code> 下载预训练参数</h3><p>除了自己载入已经保存的 我们还可以用这个方法来导入下载网上的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_3_5</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg3.5 : torch.utils.model_zoo.load_url()</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torch.utils <span class="keyword">import</span> model_zoo</span><br><span class="line">  <span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line">  model_alexnet = models.alexnet()</span><br><span class="line">  state_dict = model_zoo.load_url(<span class="string">&#x27;http://download.pytorch.org/models/alexnet-owt-7be5be79.pth&#x27;</span>)</span><br><span class="line">  model_alexnet.load_state_dict(state_dict)</span><br><span class="line"></span><br><span class="line">eg_3_5()</span><br></pre></td></tr></table></figure>
<h2 id="4-optimizer"><a href="#4-optimizer" class="headerlink" title="# 4_optimizer"></a><code># 4_optimizer</code></h2><h3 id="4-0-调用-torch-optim-模块中的优化器"><a href="#4-0-调用-torch-optim-模块中的优化器" class="headerlink" title="4.0 调用 torch.optim 模块中的优化器"></a>4.0 调用 <code>torch.optim</code> 模块中的优化器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models, transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets.mnist <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">  [</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=(<span class="number">0.5</span>,), std=(<span class="number">0.5</span>,))</span><br><span class="line">  ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_dataset = MNIST(root=<span class="string">&quot;./mnist_data&quot;</span>,</span><br><span class="line">                      train=<span class="literal">True</span>,</span><br><span class="line">                      transform=transform,</span><br><span class="line">                      target_transform=<span class="literal">None</span>,</span><br><span class="line">                      download=<span class="literal">False</span>)</span><br><span class="line">train_loader = DataLoader(dataset=train_dataset,</span><br><span class="line">                          batch_size=<span class="number">10000</span>,</span><br><span class="line">                          shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="built_in">super</span>(SimpleModel, self).__init__()</span><br><span class="line">      self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">3</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      self.conv2 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">5</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      self.flatten = nn.Flatten(start_dim=<span class="number">1</span>, end_dim=-<span class="number">1</span>)</span><br><span class="line">      self.linear = nn.Linear(in_features=<span class="number">5</span>*<span class="number">28</span>*<span class="number">28</span>, out_features=<span class="number">10</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">      x = self.conv1(x)</span><br><span class="line">      x = self.relu(x)</span><br><span class="line">      x = self.conv2(x)</span><br><span class="line">      x = self.relu(x)</span><br><span class="line">      x = self.flatten(x)</span><br><span class="line">      x = self.linear(x)</span><br><span class="line">      x = self.relu(x)</span><br><span class="line">      <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = SimpleModel()</span><br></pre></td></tr></table></figure>
<p>关键就是要使用torch.optim</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_4_0</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg4.0 : torch.optim</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line">  optimizer = optim.SGD(params=model.parameters(), lr=<span class="number">0.0001</span>, momentum=<span class="number">0.9</span>)<span class="comment">#随机梯度下降</span></span><br><span class="line">  <span class="comment">#lr 学习率</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;optim.state_dict(): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.state_dict()))<span class="comment">#打印信息</span></span><br><span class="line"></span><br><span class="line">eg_4_0()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optim.state_dict(): &#123;&#x27;state&#x27;: &#123;&#125;, &#x27;param_groups&#x27;: [&#123;&#x27;lr&#x27;: 0.0001, &#x27;momentum&#x27;: 0.9, &#x27;dampening&#x27;: 0, &#x27;weight_decay&#x27;: 0, &#x27;nesterov&#x27;: False, &#x27;params&#x27;: [0, 1, 2, 3, 4]&#125;]&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-1-注意-params参数"><a href="#4-1-注意-params参数" class="headerlink" title="4.1 注意 params参数"></a>4.1 注意 <code>params</code>参数</h3><p>我们可以让我们的网络只学习部分参数，利用params来决定</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_4_1</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg4.1 : params</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line">  params = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="string">&quot;.bias&quot;</span> <span class="keyword">in</span> name]</span><br><span class="line">  optimizer = optim.SGD(params=params, lr=<span class="number">0.0001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;optim.state_dict(): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.state_dict()))</span><br><span class="line"></span><br><span class="line">eg_4_1()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optim.state_dict(): &#123;&#x27;state&#x27;: &#123;&#125;, &#x27;param_groups&#x27;: [&#123;&#x27;lr&#x27;: 0.0001, &#x27;momentum&#x27;: 0.9, &#x27;dampening&#x27;: 0, &#x27;weight_decay&#x27;: 0, &#x27;nesterov&#x27;: False, &#x27;params&#x27;: [0, 1]&#125;]&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-2-通过-optimizer-zero-grad-loss-backward-optimizer-step-开始训练"><a href="#4-2-通过-optimizer-zero-grad-loss-backward-optimizer-step-开始训练" class="headerlink" title="4.2 通过 optimizer.zero_grad() loss.backward() optimizer.step() 开始训练"></a>4.2 通过 <code>optimizer.zero_grad()</code> <code>loss.backward()</code> <code>optimizer.step()</code> 开始训练</h3><p>先把优化器梯度清空利用zero_grad，然后计算损失函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eg_4_2</span>():</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Eg4.2 : zero_grad(), step()</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line">  <span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line">  optimizer = optim.SGD(params=model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">  loss_fn = nn.CrossEntropyLoss()<span class="comment">#从nn模块选个损失函数</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">with</span> tqdm(train_loader, desc=<span class="string">&quot;EPOCH: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch)) <span class="keyword">as</span> train_bar:</span><br><span class="line">      <span class="keyword">for</span> (x, y) <span class="keyword">in</span> train_bar:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss = loss_fn(model(x), y)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epoch: &#123;&#125;,  loss: &#123;:.6f&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss))</span><br><span class="line"></span><br><span class="line">eg_4_2()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">EPOCH: 0: 100%|██████████| 6/6 [00:29&lt;00:00,  4.87s/it]</span><br><span class="line">EPOCH: 1:   0%|          | 0/6 [00:00&lt;?, ?it/s]</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">epoch: 0,  loss: 2.300301</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EPOCH: 1: 100%|██████████| 6/6 [00:30&lt;00:00,  5.02s/it]</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">epoch: 1,  loss: 2.300429</span><br></pre></td></tr></table></figure>
<h2 id="5-train"><a href="#5-train" class="headerlink" title="# 5_train"></a><code># 5_train</code></h2><ul>
<li><p>完整模型代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=UTF-8</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  5. train</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models, transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets.mnist <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">  [</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=(<span class="number">0.5</span>,), std=(<span class="number">0.5</span>,))</span><br><span class="line">  ]</span><br><span class="line">)</span><br><span class="line"><span class="comment"># dataset</span></span><br><span class="line">train_dataset = MNIST(root=<span class="string">&quot;./mnist_data&quot;</span>,</span><br><span class="line">                      train=<span class="literal">True</span>,</span><br><span class="line">                      transform=transform,</span><br><span class="line">                      target_transform=<span class="literal">None</span>,</span><br><span class="line">                      download=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># dataloader</span></span><br><span class="line">train_loader = DataLoader(dataset=train_dataset,</span><br><span class="line">                          batch_size=<span class="number">100</span>,</span><br><span class="line">                          shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="built_in">super</span>(SimpleModel, self).__init__()</span><br><span class="line">      self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">3</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      self.conv2 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">5</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      self.flatten = nn.Flatten(start_dim=<span class="number">1</span>, end_dim=-<span class="number">1</span>)</span><br><span class="line">      self.linear = nn.Linear(in_features=<span class="number">5</span>*<span class="number">28</span>*<span class="number">28</span>, out_features=<span class="number">10</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">      x = self.conv1(x)</span><br><span class="line">      x = self.relu(x)</span><br><span class="line">      x = self.conv2(x)</span><br><span class="line">      x = self.relu(x)</span><br><span class="line">      x = self.flatten(x)</span><br><span class="line">      x = self.linear(x)</span><br><span class="line">      x = self.relu(x)</span><br><span class="line">      <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">model = SimpleModel()</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&quot;./model_2021_11_19.pth&quot;</span>))</span><br><span class="line"><span class="comment"># optimizer</span></span><br><span class="line">optimizer = optim.SGD(params=model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># train</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">  <span class="keyword">with</span> tqdm(train_loader, desc=<span class="string">&quot;EPOCH: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch)) <span class="keyword">as</span> train_bar:</span><br><span class="line">    <span class="keyword">for</span> (x, y) <span class="keyword">in</span> train_bar:</span><br><span class="line">      optimizer.zero_grad()</span><br><span class="line">      loss = loss_fn(model(x), y)</span><br><span class="line">      loss.backward()</span><br><span class="line">      optimizer.step()</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;epoch: &#123;&#125;,  loss: &#123;:.6f&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss))</span><br><span class="line"></span><br><span class="line">time = <span class="built_in">str</span>(datetime.now()).split(<span class="string">&quot; &quot;</span>)[<span class="number">0</span>].replace(<span class="string">&quot;-&quot;</span>, <span class="string">&quot;_&quot;</span>)</span><br><span class="line">torch.save(model.state_dict(), <span class="string">&quot;model_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(time))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;~~~~~~撒花~~~~~~&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2022/05/07/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E2%85%A2/" rel="next" title="论文学习Ⅲ">
                <i class="fa fa-chevron-left"></i> 论文学习Ⅲ
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2022/05/07/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E2%85%A3/" rel="prev" title="论文学习Ⅳ">
                论文学习Ⅳ <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="2822328563@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-fa fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Pytorch%E5%AE%9E%E6%88%98"><span class="nav-number">1.</span> <span class="nav-text">Pytorch实战</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-dataset"><span class="nav-number">1.1.</span> <span class="nav-text"># 1_dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-0-%E7%BB%A7%E6%89%BF-torch-utils-data-Dataset"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.0 继承 torch.utils.data.Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E5%AE%9E%E7%8E%B0-getitem-%E5%92%8C-len-%E4%B8%A4%E4%B8%AAmagic-methods"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.1 实现 __getitem__ 和 __len__ 两个magic methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E7%90%86%E8%A7%A3-MNIST-%E7%B1%BB%EF%BC%8C%E4%BB%A5%E5%8F%8A-transforms-%E6%A8%A1%E5%9D%97"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.2 理解 MNIST 类，以及 transforms 模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E5%88%A9%E7%94%A8-torchvision-datasets-%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.3 利用 torchvision.datasets 中的数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-%E7%90%86%E8%A7%A3-ImageFolder-%E7%B1%BB%E5%8F%8A%E5%85%B6-classes-%E4%B8%8E-class-to-idx-%E5%B1%9E%E6%80%A7"><span class="nav-number">1.1.5.</span> <span class="nav-text">1.4 理解 ImageFolder 类及其 classes 与 class_to_idx 属性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-dataloader"><span class="nav-number">1.2.</span> <span class="nav-text"># 2_dataloader</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-0-%E5%88%A9%E7%94%A8-torch-utils-data-DataLoader%E7%B1%BB"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.0 利用 torch.utils.data.DataLoader类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E7%90%86%E8%A7%A3-iter-%E8%BF%99%E4%B8%AAmagic-method"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.1 理解 __iter__ 这个magic method</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E5%8C%BA%E5%88%86-Dataloader-%E4%B8%8E-Dataset-%E7%9A%84-len"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.2 区分 Dataloader 与 Dataset 的 __len__</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%88%A9%E7%94%A8-%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0-enumerate-%E4%B8%8E-tqdm-%E6%A8%A1%E5%9D%97"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.3 利用 内置函数 enumerate 与 tqdm 模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%8F%A6%E5%A4%96%E6%9C%89%E9%9C%80%E8%A6%81%E5%8F%AF%E4%BB%A5%E6%9B%B4%E6%94%B9-collate-fn"><span class="nav-number">1.2.5.</span> <span class="nav-text">2.3 另外有需要可以更改 collate_fn</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-model"><span class="nav-number">1.3.</span> <span class="nav-text"># 3_model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-0-%E7%BB%A7%E6%89%BF-torch-nn-Module%EF%BC%8C%E6%B3%A8%E6%84%8F-super-init"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.0 继承 torch.nn.Module，注意 super().__init__()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E7%90%86%E8%A7%A3-call-%E8%BF%99%E4%B8%AAmagic-method-%E4%B8%8E%E8%87%AA%E5%AE%9A%E4%B9%89-forward-%E5%85%B3%E7%B3%BB"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.1 理解 __call__ 这个magic method 与自定义 forward 关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E6%B3%A8%E6%84%8F-PyTorch-%E4%B8%AD%E6%95%B0%E6%8D%AE%E7%9A%84%E6%91%86%E6%94%BE-B-C-H-W"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.2 注意 PyTorch 中数据的摆放 (B, C, H ,W)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E8%B0%83%E7%94%A8-torchvison-models-%E4%B8%AD%E7%8E%B0%E6%88%90%E7%9A%84%E7%BD%91%E7%BB%9C"><span class="nav-number">1.3.4.</span> <span class="nav-text">3.3 调用 torchvison.models 中现成的网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-%E6%B3%A8%E6%84%8F-torch-nn-Module-dict-state-torch-save-torch-load-%E4%BB%A5%E5%8F%8A-torch-nn-Module-load-state-dict-%E5%8F%8A%E5%85%B6%E4%B8%AD%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.5.</span> <span class="nav-text">3.4 注意 torch.nn.Module.dict_state() torch.save() torch.load() 以及 torch.nn.Module.load_state_dict() 及其中参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-%E5%88%A9%E7%94%A8-torch-utils-model-zoo-load-url-%E4%B8%8B%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.6.</span> <span class="nav-text">3.5 利用 torch.utils.model_zoo.load_url() 下载预训练参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-optimizer"><span class="nav-number">1.4.</span> <span class="nav-text"># 4_optimizer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-0-%E8%B0%83%E7%94%A8-torch-optim-%E6%A8%A1%E5%9D%97%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.0 调用 torch.optim 模块中的优化器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E6%B3%A8%E6%84%8F-params%E5%8F%82%E6%95%B0"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.1 注意 params参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E9%80%9A%E8%BF%87-optimizer-zero-grad-loss-backward-optimizer-step-%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">1.4.3.</span> <span class="nav-text">4.2 通过 optimizer.zero_grad() loss.backward() optimizer.step() 开始训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-train"><span class="nav-number">1.5.</span> <span class="nav-text"># 5_train</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yinghao Wang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"live2d-widget-model-haruto"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
